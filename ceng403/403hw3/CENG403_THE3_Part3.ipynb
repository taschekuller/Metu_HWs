{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1X4X2hmaEmip"
      },
      "source": [
        "# CENG403 - Spring 2024 - THE3\n",
        "# Task 3: Finetune ResNet\n",
        "\n",
        "In this task, we will practice transfer learning by adapting and finetuning ResNet18 (pretrained on ImageNet) for CIFAR10.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KyHtEWkOuZW"
      },
      "source": [
        "## 3.1 Download and Test Pretrained ResNet18\n",
        "\n",
        "We have talked about ResNet in detail in the lectures. We will take Pytorch's ResNet18 model (https://pytorch.org/hub/pytorch_vision_resnet/), which was trained on ImageNet, and adapt & finetune it for CIFAR10.\n",
        "\n",
        "Here are a couple of critical information about ResNet18's input (from Pytorch docs):\n",
        "\n",
        "<i>\"All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\"</i>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWxyCVI_OTdi"
      },
      "source": [
        "### 3.1.1 Download ResNet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-K1Qp-TV7Npz",
        "outputId": "ad853b42-62e6-4778-f50e-5d1ae9b083f8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /Users/dogukanince/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Thanks to Pytorch datasets, this is very easy:\n",
        "\n",
        "OrigResNet18 = None\n",
        "###########################################################\n",
        "# @TODO: Look at PyTorch's documentation for downloading  #\n",
        "# and loading a pretrained ResNet18 model.                #\n",
        "#                                                         #\n",
        "# Hint: This should be a single function call.            #\n",
        "###########################################################\n",
        "import torch\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "OrigResNet18 = model\n",
        "model.eval()\n",
        "\n",
        "###########################################################\n",
        "#                         END OF YOUR CODE                #\n",
        "###########################################################\n",
        "print(OrigResNet18)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bDj9IQRL7ms"
      },
      "source": [
        "### 3.1.2 Get ImageNet Labels\n",
        "\n",
        "Before we finetune ResNet18, let us see what it predicts on CIFAR10. To be able to do so, we will require the list of ImageNet labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "EnCnO18N7Scy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: get\n"
          ]
        }
      ],
      "source": [
        "# Download ImageNet labels\n",
        "!wget -nc https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
        "\n",
        "# Read the categories\n",
        "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
        "    categories = [s.strip() for s in f.readlines()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk6K0jC_OZs0"
      },
      "source": [
        "### 3.1.3 Load CIFAR10\n",
        "\n",
        "As we mentioned above, ResNet18 expects images with resolution 224x224 and normalized with a mean & std. Therefore, while loading CIFAR10, we will apply certain transformations to handle these requirements. (Note that this cell is slightly different than 2.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HRQY0yM7oTh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "# import collections\n",
        "\n",
        "# torch._six = collections\n",
        "# torch._six.string_classes = (str,)\n",
        "### Upper part solved torch has no attribute named _six error  ###\n",
        "\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "TF = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=TF)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=TF)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "CIFAR10_classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUxUQKZSO3q2"
      },
      "source": [
        "### 3.1.4 Apply Pre-trained ResNet18 on CIFAR10\n",
        "\n",
        "Let us look at what ResNet18 predicts on a batch of CIFAR10. For a set of samples, we will visualize the images and look at ResNet18's top predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "KbITob-576pc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
            "    exitcode = _main(fd, parent_sentinel)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
            "    self = reduction.pickle.load(from_parent)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages/torchvision/__init__.py\", line 6, in <module>\n",
            "    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils\n",
            "  File \"/Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages/torchvision/_meta_registrations.py\", line 163, in <module>\n",
            "    @torch._custom_ops.impl_abstract(\"torchvision::nms\")\n",
            "     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages/torch/library.py\", line 654, in register\n",
            "    use_lib._register_fake(op_name, func, _stacklevel=stacklevel + 1)\n",
            "  File \"/Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages/torch/library.py\", line 154, in _register_fake\n",
            "    handle = entry.abstract_impl.register(func_to_register, source)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages/torch/_library/abstract_impl.py\", line 31, in register\n",
            "    if torch._C._dispatch_has_kernel_for_dispatch_key(self.qualname, \"Meta\"):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: operator torchvision::nms does not exist\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[36], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Get a batch\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m dataiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(testloader)\n\u001b[1;32m      6\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataiter)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Get scores for classes on the batch\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:440\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_iterator()\n",
            "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[38;5;28mself\u001b[39m)\n",
            "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1038\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1031\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1038\u001b[0m w\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
            "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.12/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Popen(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.12/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_context\u001b[38;5;241m.\u001b[39mget_context()\u001b[38;5;241m.\u001b[39mProcess\u001b[38;5;241m.\u001b[39m_Popen(process_obj)\n",
            "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.12/multiprocessing/context.py:289\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Popen(process_obj)\n",
            "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.12/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(process_obj)\n",
            "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.12/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_launch(process_obj)\n",
            "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.12/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(fp\u001b[38;5;241m.\u001b[39mgetbuffer())\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Get a batch\n",
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Get scores for classes on the batch\n",
        "with torch.no_grad():\n",
        "    output = OrigResNet18(images)\n",
        "\n",
        "# Convert them to probabilities (shape: batch_size x 1000)\n",
        "probabilities = torch.nn.functional.softmax(output, dim=1)\n",
        "\n",
        "# Show results on a 2x2 grid\n",
        "S=2\n",
        "for i in range(S):\n",
        "  for j in range(S):\n",
        "    X = images[i*S+j]\n",
        "    X = np.transpose((X.numpy()/2+0.5), (1, 2, 0))\n",
        "    top1_prob, top1_catid = torch.topk(probabilities[i*S+j], 1)\n",
        "    title = \"{} p:{:1.2f}\".format(categories[top1_catid], top1_prob.item())\n",
        "\n",
        "    plt.subplot(S, S, i*S+j+1)\n",
        "    plt.imshow(X)\n",
        "    plt.axis('off')\n",
        "    plt.title(title)\n",
        "    plt.subplots_adjust(hspace = 0.5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrHglujl79zy"
      },
      "source": [
        "We see that the predictions are way off and we will hopefully get better results with some finetuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV9omiSefxOU"
      },
      "source": [
        "## 3.2 Adapt ResNet18 for CIFAR10\n",
        "\n",
        "We will \"freeze\" the parameters of ResNet18 and replace the last layer of ResNet18 with a new layer which we will finetune."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXcniP0b8-Xk"
      },
      "outputs": [],
      "source": [
        "# Copy ResNet18\n",
        "import copy\n",
        "NewResNet18 = copy.deepcopy(OrigResNet18)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTt3F-EHI1_9"
      },
      "source": [
        "### 3.2.1 Freeze Parameters of ResNet18\n",
        "\n",
        "We \"freeze\" a parameter by setting its `requires_grad` member variable to `False`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lU8p7OE9D5N"
      },
      "outputs": [],
      "source": [
        "###########################################################\n",
        "# @TODO: Go over the parameters of NewResNet18 and set    #\n",
        "# requires_grad for all parameters to False.              #\n",
        "#                                                         #\n",
        "# Hint: Check parameters() member function of NewResNet18.#\n",
        "###########################################################\n",
        "NewResNet18.fc.requires_grad = False\n",
        "\n",
        "###########################################################\n",
        "#                         END OF YOUR CODE                #\n",
        "###########################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htbhG_H5I53Z"
      },
      "source": [
        "### 3.2.2 Add a New Learnable FC Layer to ResNet18\n",
        "\n",
        "If you look at the summary of ResNet18 shown above, you will see that the last layer is:\n",
        "\n",
        "  `(fc): Linear(in_features=512, out_features=1000, bias=True)`\n",
        "\n",
        "In our case, we should just replace this with a new FC layer (its dimensions should be straight-forward to figure out)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxsng68c9KMF"
      },
      "outputs": [],
      "source": [
        "NewResNet18.fc = None\n",
        "###########################################################\n",
        "# @TODO: Create a new layer and save it into              #\n",
        "# NewResNet18.fc. This new layer will map the activations #\n",
        "# of the previous layer to the outputs for the CIFAR10    #\n",
        "# classes.                                                #\n",
        "###########################################################\n",
        "import torch.nn as nn\n",
        "NewResNet18.fc = nn.Linear(512, 10)\n",
        "batch_size = 8\n",
        "batch = next(iter(trainloader))\n",
        "images, labels = batch\n",
        "output = NewResNet18(images)\n",
        "assert output.shape == torch.Size([batch_size, 10]), output.shape\n",
        "\n",
        "\n",
        "\n",
        "###########################################################\n",
        "#                         END OF YOUR CODE                #\n",
        "###########################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQic000dKCBi"
      },
      "source": [
        "### 3.2.3 Visualize the Model\n",
        "\n",
        "Now, let us see whether the new fc layer is correct for CIFAR10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Caug6kMO9Owh"
      },
      "outputs": [],
      "source": [
        "print(NewResNet18.fc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8VqkhyAf1s4"
      },
      "source": [
        "## 3.3 Finetune ResNet18\n",
        "\n",
        "While finetuning ResNet18, we will just update the last layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6Xwis1-J26R"
      },
      "source": [
        "### 3.3.1 Training Method\n",
        "\n",
        "This is the same training method from Task 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdZj92vK9UR-"
      },
      "outputs": [],
      "source": [
        "def train(model, criterion, optimizer, epochs, dataloader, verbose=True):\n",
        "  \"\"\"\n",
        "    Define the trainer function. We can use this for training any model.\n",
        "    The parameter names are self-explanatory.\n",
        "\n",
        "    Returns: the loss history.\n",
        "  \"\"\"\n",
        "  \n",
        "  loss_history = []\n",
        "  for epoch in range(epochs):\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "\n",
        "      # Our batch:\n",
        "      inputs, labels = data\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # zero the gradients as PyTorch accumulates them\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Obtain the scores\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      # Calculate loss\n",
        "      loss = criterion(outputs.to(device), labels)\n",
        "\n",
        "      # Backpropagate\n",
        "      loss.backward()\n",
        "\n",
        "      # Update the weights\n",
        "      optimizer.step()\n",
        "\n",
        "      loss_history.append(loss.item())\n",
        "      \n",
        "\n",
        "    if verbose: print(f'Epoch {epoch} / {epochs}: avg. loss of last 5 iterations {np.sum(loss_history[:-6:-1])/5}')\n",
        "\n",
        "  return loss_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15VCbg0lKPIz"
      },
      "source": [
        "### 3.3.2 Finetune the Adapted ResNet18 on CIFAR10\n",
        "\n",
        "We will only provide the learnable parameters to the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30fapGbT9XYZ"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# For reproducibility, let us recreate the FC layer here with a fixed seed:\n",
        "torch.manual_seed(403)\n",
        "random.seed(403)\n",
        "np.random.seed(403)\n",
        "\n",
        "NewResNet18.fc = None\n",
        "###########################################################\n",
        "# @TODO: Repeat what you did in 3.2.2 here                  #\n",
        "###########################################################\n",
        "\n",
        "\n",
        "###########################################################\n",
        "#                         END OF YOUR CODE                #\n",
        "###########################################################\n",
        "\n",
        "def get_learnable_parameters(model):\n",
        "    params_to_update = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "    return params_to_update\n",
        "\n",
        "parameters_to_update = get_learnable_parameters(NewResNet18)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(parameters_to_update, lr=0.0001, momentum=0.95)\n",
        "\n",
        "NewResNet18 = NewResNet18.to(device)\n",
        "epochs = 2\n",
        "loss_history = train(NewResNet18, criterion, optimizer, epochs, trainloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SUT6gj9KenX"
      },
      "source": [
        "### 3.3.3 The Loss Curve\n",
        "\n",
        "You will see that the loss curve is very noisy, which suggests that we should finetune our hyper-parameters. Though, we will see that we get already reasonably well performance on test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bikuvprP9bVf"
      },
      "outputs": [],
      "source": [
        "plt.plot(loss_history)\n",
        "plt.xlabel('Iteration number')\n",
        "plt.ylabel('Loss value')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cBp7Xu_Kkhp"
      },
      "source": [
        "### 3.3.4 Quantitative Results\n",
        "\n",
        "We can analyze the accuracy of the predictions as follows. You should see around 69\\% accuracies. We can finetune the hyperparameters to obtain better results.\n",
        "\n",
        "*Disclaimer: This code piece is taken from PyTorch examples.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXUwF4ff9e9O"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "NewResNet18 = NewResNet18.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = NewResNet18(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69QV-5JKVHVl"
      },
      "source": [
        "### 3.3.5 Visual Results\n",
        "\n",
        "We see that with just two epochs of training a single FC layer, we can get decent results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1gI76tb9iUV"
      },
      "outputs": [],
      "source": [
        "# Get a batch\n",
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "images = images.to(device)\n",
        "labels = labels.to(device)\n",
        "\n",
        "# Get scores for classes on the batch\n",
        "with torch.no_grad():\n",
        "    output = NewResNet18(images)\n",
        "\n",
        "# Convert them to probabilities (shape: batch_size x 1000)\n",
        "probabilities = torch.nn.functional.softmax(output, dim=1)\n",
        "\n",
        "# Show results on a 2x2 grid\n",
        "S=2\n",
        "for i in range(S):\n",
        "  for j in range(S):\n",
        "    X = images[i*S+j]\n",
        "    X = np.transpose((X.to(\"cpu\").numpy()/2+0.5), (1, 2, 0))\n",
        "    top1_prob, top1_catid = torch.topk(probabilities[i*S+j], 1)\n",
        "    title = \"{} p:{:1.2f}\".format(CIFAR10_classes[top1_catid], top1_prob.item())\n",
        "\n",
        "    plt.subplot(S, S, i*S+j+1)\n",
        "    plt.imshow(X)\n",
        "    plt.axis('off')\n",
        "    plt.title(title)\n",
        "    plt.subplots_adjust(hspace = 0.5)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
