{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b0adfc5d431b9a8",
   "metadata": {},
   "source": [
    "# Lang Chain\n",
    "\n",
    "## What is LangChain\n",
    "LangChain is an open-source framework that helps developers connect LLMs, data sources, and other functionality under a single, unified syntax. With LangChain, developers can create scalable, modular LLM applications with greater ease. This course will cover LangChain in Python, but libraries also exist for JavaScript.\n",
    "\n",
    "LangChain encompasses an entire ecosystem of tools, but in this tutorial, we'll focus on the core components of the LangChain library: \n",
    "* LLMs, including open-source and proprietary models, \n",
    "* prompts, \n",
    "* chains, \n",
    "* agents, and \n",
    "* document retrievers. \n",
    " \n",
    "To install LangChain please run the following code in the Terminal: `conda install conda-forge::langchain`\n",
    "\n",
    "Once installed you can import it using the following import statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6829e501750002ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from venv import create\n",
    "\n",
    "import langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefc6d4919ce6b8f",
   "metadata": {},
   "source": [
    "# 1. Prompt Templates\n",
    "\n",
    "Prompt templates help to translate user input and parameters into instructions for a language model. This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.\n",
    "\n",
    "Prompt Templates take as input a dictionary, where each key represents a variable in the prompt template to fill in.\n",
    "\n",
    "Prompt Templates output a `PromptValue`. This `PromptValue` can be passed to an LLM or a ChatModel, and can also be cast to a string or a list of messages. The reason why PromptValue exists is to make it easy to switch between strings and messages.\n",
    "\n",
    "There are a few different types of prompt templates:\n",
    "\n",
    "* StringPromptTemplates\n",
    "* ChatPromptTemplates\n",
    "* MessagesPlaceholder\n",
    "\n",
    "## 1.1. String Prompt Templates\n",
    "\n",
    "These prompt templates are used to format a single string, and generally are used for simpler inputs. For example, a common way to construct and use a PromptTemplate is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86cdae8d95cfb8de",
   "metadata": {},
   "source": [
    "## 1.2. ChatPromptTemplates\n",
    "\n",
    "These prompt templates are used to format a list of messages. These \"templates\" consist of a list of templates themselves. For example, a common way to construct and use a `ChatPromptTemplate` is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dc7e0dc6f3aa3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "298706c379514054",
   "metadata": {},
   "source": [
    "In the above example, this ChatPromptTemplate will construct two messages when called. The first is a system message, that has no variables to format. The second is a HumanMessage, and will be formatted by the topic variable the user passes in.\n",
    "\n",
    "## 1.3. MessagesPlaceholder\n",
    "\n",
    "This prompt template is responsible for adding a list of messages in a particular place. In the above `ChatPromptTemplate`, we saw how we could format two messages, each one a string. But what if we wanted the user to pass in a list of messages that we would slot into a particular spot? This is how you use `MessagesPlaceholder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1b84d3117666d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d1e8a735fa3080f",
   "metadata": {},
   "source": [
    "This will produce a list of two messages, the first one being a system message, and the second one being the HumanMessage we passed in. If we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5 passed in). This is useful for letting a list of messages be slotted into a particular spot.\n",
    "\n",
    "## 2. How to use few shot examples\n",
    "\n",
    "You can create a simple prompt template that provides the model with example inputs and outputs when generating. Providing the LLM with a few such examples is called few-shotting, and is a simple yet powerful way to guide generation and in some cases drastically improve model performance.\n",
    "\n",
    "> Few-shotting is a technique in LLM prompting where you provide a small number of examples within the prompt itself to guide the model towards the desired output.  Essentially, you're demonstrating the task you want the model to perform by showing it a few input-output pairs. This helps the LLM understand the pattern and generate more accurate and relevant responses\n",
    "\n",
    "Below is an example of few-shot prompt:\n",
    "\n",
    "```\n",
    "Generate a creative product name for a new line of sunglasses.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Input: Sleek, futuristic design with polarized lenses\n",
    "Output: \"Solaris Eclipse\"\n",
    "\n",
    "Input: Classic aviator style with a modern twist\n",
    "Output: \"Skybound Voyager\" \n",
    "\n",
    "Input: Oversized frames with vibrant colors\n",
    "Output: \"Chromatic Dream\" \n",
    "```\n",
    "\n",
    "We will cover few-shotting with string prompt templates.\n",
    "\n",
    "Create  a formatter that will format the few-shot examples into a string. This formatter should be a `PromptTemplate` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d2a59d79dcea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da5829e33978a06a",
   "metadata": {},
   "source": [
    "Next, we'll create a list of few-shot examples. Each example should be a dictionary representing an example input to the formatter prompt we defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9387e8bf3eb706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",\n",
    "        \"answer\": \"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: How old was Muhammad Ali when he died?\n",
    "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
    "Follow up: How old was Alan Turing when he died?\n",
    "Intermediate answer: Alan Turing was 41 years old when he died.\n",
    "So the final answer is: Muhammad Ali\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"When was the founder of craigslist born?\",\n",
    "        \"answer\": \"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: Who was the founder of craigslist?\n",
    "Intermediate answer: Craigslist was founded by Craig Newmark.\n",
    "Follow up: When was Craig Newmark born?\n",
    "Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
    "So the final answer is: December 6, 1952\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Who was the maternal grandfather of George Washington?\",\n",
    "        \"answer\": \"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: Who was the mother of George Washington?\n",
    "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
    "Follow up: Who was the father of Mary Ball Washington?\n",
    "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
    "So the final answer is: Joseph Ball\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\",\n",
    "        \"answer\": \"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: Who is the director of Jaws?\n",
    "Intermediate Answer: The director of Jaws is Steven Spielberg.\n",
    "Follow up: Where is Steven Spielberg from?\n",
    "Intermediate Answer: The United States.\n",
    "Follow up: Who is the director of Casino Royale?\n",
    "Intermediate Answer: The director of Casino Royale is Martin Campbell.\n",
    "Follow up: Where is Martin Campbell from?\n",
    "Intermediate Answer: New Zealand.\n",
    "So the final answer is: No\n",
    "\"\"\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bddbb36830286ab",
   "metadata": {},
   "source": [
    "Let's test the formatting prompt with one of our examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88ed24eb5f2e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75b87655c18b974c",
   "metadata": {},
   "source": [
    "Finally, create a `FewShotPromptTemplate` object. This object takes in the few-shot examples and the formatter for the few-shot examples. When this `FewShotPromptTemplate` is formatted, it formats the passed examples using the `example_prompt`, then and adds them to the final prompt before suffix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa879f29362e7d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce38305069d07eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da22caeb3047e703",
   "metadata": {},
   "source": [
    "Now, let's create a `FewShotPromptTemplate` object. This object takes in the example selector and the formatter prompt for the few-shot examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40359537838d34ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d48996f74fe4981",
   "metadata": {},
   "source": [
    "`FewShotPromptTemplate` is a more general-purpose template. It simply formats the examples and appends the user input as a string. This is suitable for models that don't necessarily expect a chat-like interaction. The output is a single string containing the model's response.\n",
    "\n",
    "`FewShotChatMessagePromptTemplate` is designed specifically for chat-based language models. It structures the prompt as a conversation, with alternating \"human\" and \"AI\" messages. This mimics the way users interact with chatbots, making the prompt more natural and potentially leading to better responses. The output is also expected to be in the form of chat messages.\n",
    "\n",
    "Let's rewrite the code above with `FewShotChatMessagePromptTemplate`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f765cd0d4346333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11b70f7f55dad376",
   "metadata": {},
   "source": [
    "And we can pass this few-shot chat message prompt template into another chat prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e958ab7197b2e02b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a98fa0fac16f9ee",
   "metadata": {},
   "source": [
    "Then, we can use the few shot examples in chat models. Let's first create a simple model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1695340577b1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "HF_API = 'xyz'\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id='tiiuae/falcon-7b-instruct',\n",
    "    huggingfacehub_api_token =HF_API,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10de64ddadf4b83",
   "metadata": {},
   "source": [
    "Finally, you can connect your model to the few-shot prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca397d8d89adaf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db93e0edf28f4fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58027bee791d7a80",
   "metadata": {},
   "source": [
    "Let's try this example with Groq:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2602ffbfb0de00c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "GROQAPI = os.getenv(\"GROQ_API_KEY\")\n",
    "GROQAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc3d62ea5e5316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f82a56a4a8d205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1801d3dc800359",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222dc5fae01918bf",
   "metadata": {},
   "source": [
    "# 3. Managing Chat Model Memory\n",
    "\n",
    "Managing memory is important for conversations with chat models; it opens up the possibility of providing follow-up questions, of building and iterating on model responses, and for adaptation to the user's preferences and behaviors. Although LangChain allows us to customize and optimize in-conversation chatbot memory, it is still limited by the model's context window. An LLM's context window is the amount of input text the model can consider at once when generating a response, and the length of this window varies for different models. \n",
    "\n",
    "ChatMessageHistory stores the full history of messages between the user and model. By providing this to the model, we can provide follow-up questions and iterate on the response message. \n",
    "\n",
    "Let's implement this message history into a Hugging Face model. We first import the `ChatMessageHistory` and ChatOpenAI classes and define the LLM. To begin the conversation history, instantiate `ChatMessageHistory` and store it as a variable. \n",
    "\n",
    "We'll start our conversation with an AI message, specified using the `.add_ai_message()` method, which can help set the tone and direction of the conversation. We can add user messages to the history with the `.add_user_message()` method. To provide these messages to the model, invoke the model on the messages attribute of the history. The text response is stored under the `.content` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b40575bc63c934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "history.add_user_message(\"Hello! Who are you?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9af5b5e97c2f51",
   "metadata": {},
   "source": [
    "When additional user messages are provided, the model bases its response on the full context stored in the conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab515e45571a5553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34e60a03db1bc105",
   "metadata": {},
   "source": [
    "ChatMessageHistory provides a simple way to store and manage conversation history. It's handy if you just need a straightforward way to keep track of past messages. You have less direct control over how the history is formatted within the prompt.\n",
    "\n",
    "You can use `ChatPromptTemplate` to explicitly define how the conversation history and the current user input are presented to the language model. This separation makes the prompt engineering process more transparent.\n",
    "\n",
    "You can provide a `SystemMessage` to guide the LLM's overall behavior (e.g., \"You are a helpful assistant\"). This helps set the context for the conversation.\n",
    "\n",
    "Also, you have direct control over the history list. You can manipulate it (add, remove, modify messages) before passing it to the LLM, giving you fine-grained control over what the model sees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512a315f8d880d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea7572b5c3f734a5",
   "metadata": {},
   "source": [
    "Often, you need to handle multiple sessions or users. With the above approach, you'd have to manage the history list yourself, potentially saving it to a database or file. You can implement session-based history management using `RunnableWithMessageHistory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9c51350d434479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    # This function retrieves the chat history for a given session_id.\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a128341a4741a61b",
   "metadata": {},
   "source": [
    "The `get_session_history` function allows you to manage history on a per-session basis. This means you can have separate conversation histories for different users or different conversation threads.\n",
    "\n",
    "Organization with `RunnableWithMessageHistory`: This class handles the logic of:\n",
    "\n",
    "* Retrieving the correct history for a session.\n",
    "* Adding new messages to the history.\n",
    "* Passing the history to your chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242689243376bb0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31335cf289826300",
   "metadata": {},
   "source": [
    "Let's continue conversation in the same session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d927758d7bfe5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9827cbacbe7b34ba",
   "metadata": {},
   "source": [
    "Let's ask something new in a new session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de77f6ccb0daf209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ec4331189088963",
   "metadata": {},
   "source": [
    "## 4. Sequential Chains\n",
    "Some problems can only be solved sequentially. Consider a chatbot used to create a travel itinerary. We need to tell the chatbot our destination, receive suggestions on what to see on our trip, and tell the model which activities to select to compile the itinerary.\n",
    "\n",
    "This is a sequential problem, as it requires more than one user input: one to specify the destination, and another to select the activities. Let's code this out!\n",
    "\n",
    "In sequential chains, the output from one chain becomes the input to another. We'll create two prompt templates: one to generate suggestions for activities from the input destination, and another to create an itinerary for one day of activities from the model's top three suggestions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd48ab04fbcc923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_prompt = PromptTemplate(\n",
    "    input_variables = [\"destination\"],\n",
    "    template=\"I am planning a trip to {destination}. Can you suggest some activities to do there?\"\n",
    ")\n",
    "\n",
    "activities_prompt = PromptTemplate(\n",
    "    input_variables = [\"activities\"],\n",
    "    template=\"I only have one day, so can you create an itinerary from your top three activities: {activities}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d47fa374687b9a",
   "metadata": {},
   "source": [
    " We define our model, and begin our sequential chain. We start by defining a dictionary that passes our destination prompt template to the LLM and parses the output to a string, all using **LangChain Expression Language (LCEL)** pipe. This gets assigned to the \"activities\" key, which is important, as this is the input variable to the second prompt template. We pipe the first chain into the second prompt template, then into the LLM, and again, parse to a string. We also wrap the sequential chain in parentheses so we can split this code across multiple lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be6d98ac9324913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931faf24dc659eb1",
   "metadata": {},
   "source": [
    " To summarize: the `destination_prompt` is passed to the LLM to generate the activity suggestions, and the output is parsed to a string and assigned to \"activities\". This is passed to the second `activities_prompt`, which is passed to the LLM to generate the itinerary, which is parsed as a string.\n",
    "\n",
    "Let's invoke the chain, passing Rome as our input destination. The model considered that we only had one day to explore, and wove in it's top suggestions of the Colosseum and Vatican City."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a7bc290d483eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39decaa4660e7858",
   "metadata": {},
   "source": [
    "# 6. Gentle Introduction to LangChain Agents\n",
    "\n",
    "In LangChain, agents use language models to determine actions. Agents often use tools, which are functions called by the agent to interact with the system. These tools can be high-level utilities to transform inputs, or they can be task-specific. Agents can even use chains and other agents as tools. In this tutorial, we'll discuss a type of agent called **ReAct** agents.\n",
    "\n",
    "ReAct stands for *reasoning and acting*, and this is exactly how the agent operates. It prompts the model using a repeated loop of thinking, acting, and observing. If we were to ask a ReAct agent that had access to a weather tool, \"What is the weather like in Kingston, Jamaica?\", it would start by thinking about the task and which tool to call, call that tool using the information, and observe the results from the tool call.\n",
    "\n",
    "To implement agents, we'll be using **LangGraph**, which is branch of the LangChain ecosystem specifically for designing agentic systems, or systems including agents. Like LangChain's core library, it's is built to provide a unified, tool-agnostic syntax. \n",
    "\n",
    "We'll create a ReAct agent that can solve math problems - something most LLMs struggle with. We import `create_react_agent` from `langgraph` and the `load_tools()` function. We initialize our LLM, and load the llm-math tool using the `load_tools()` function. To create the agent, we pass the LLM and tools to `create_react_agent()`, Just like chains, agents can be executed with the `.invoke()` method. Here, we pass the chat model a message to find the square root of 101, which isn't a whole number. Let's see how the agent approaches the problem!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afca2459f72d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.agents import load_tools, AgentExecutor\n",
    "\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e622cde284c88c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the final answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbfaf1b41f0ab86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tools\n",
    "\n",
    "# Define the agent\n",
    "\n",
    "# Invoke the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24119b7fe7f16ff4",
   "metadata": {},
   "source": [
    "# 5. Custom Tools for Agents\n",
    "\n",
    "Now that we've created our first agent, let's take a closer a look at tools so we can design our own. **Tools** in LangChain must be formatted in a specific way to be compatible with agents. They must have a name, accessible via the `.name` attribute. A description, which is used by the LLM to determine when to call the tool. In this case, if the LLM interprets the task as being a math problem, it will likely call this tool based on its description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db199124ba5b66d",
   "metadata": {},
   "source": [
    "Let's say we want to define a Python function to generate a financial report for a company. It takes three arguments: the company_name, revenue, and expenses, and outputs a string containing the net_income. We make the use of this function clear in the docstring, defined using triple quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163451a39da932be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def financial_report(company_name:str, revenue: int, expenses: int) -> str:\n",
    "    \"\"\"Generate a financial report for a company that calculates net income.\"\"\"\n",
    "\n",
    "    net_income = revenue - expenses\n",
    "\n",
    "    report = f\"Financial Report for {company_name}:\\n\"\n",
    "    report += f\"Revenue: {revenue}\\n\"\n",
    "    report += f\"Expenses: {expenses}\\n\"\n",
    "    report += f\"Net Income: {net_income}\\n\"\n",
    "\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8be453911f1422a",
   "metadata": {},
   "source": [
    "Here's what the report looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a678e79140bcb426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98bb4e8205175d4b",
   "metadata": {},
   "source": [
    "Let's convert this function into a tool our agent can call. To do this, we import the `@tool` decorator and add it before the function definition. Don't worry if you're not familiar with Python decorators; the `@tool` modifies the function so it's in the correct format to be used by a tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2001168f4aa8c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "def financial_report(company_name:str, revenue: int, expenses: int) -> str:\n",
    "    \"\"\"Generate a financial report for a company that calculates net income.\"\"\"\n",
    "\n",
    "    net_income = revenue - expenses\n",
    "\n",
    "    report = f\"Financial Report for {company_name}:\\n\"\n",
    "    report += f\"Revenue: {revenue}\\n\"\n",
    "    report += f\"Expenses: {expenses}\\n\"\n",
    "    report += f\"Net Income: {net_income}\\n\"\n",
    "\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b30d77d18f0cca",
   "metadata": {},
   "source": [
    "Like with the built-in tool we were looking at, we can now examine the various attributes of our tool. These include its name, which is the function name by default, its description, which is the function's docstring, and return_direct, which is set to False by default.\n",
    "\n",
    "When a tool is called with `return_direct=True`, the agent immediately returns the tool's output to the user as the final answer. You should use this option mostly when a tool's output is already in a suitable format for the user and doesn't require further processing or interpretation by the agent.\n",
    "\n",
    "\n",
    " \n",
    "We can also print the tools arguments, which lay out the argument names and expected data types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6f7a31ba503c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(financial_report.name)\n",
    "print(financial_report.description)\n",
    "print(financial_report.return_direct)\n",
    "print(financial_report.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f07a4a5a10ce9a",
   "metadata": {},
   "source": [
    "\n",
    "Let's put our tool into action! We'll again use a ReAct agent, combining the chat LLM with a list of tools to use, containing our new custom tool. We invoke the agent with an input containing the required information: a company name, revenue, and expenses. The response from the agent starts with our input, then determines that the financial_report tool should be called, which returns a tool message containing the output from our function, and finally, the output is passed to the LLM, which responds to us. Let's zoom in on this final message.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8ec821207be0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "547a58b228aa030a",
   "metadata": {},
   "source": [
    "Here's the final output from the LLM. Notice anything? Here's the output from the tool based on the function we defined. Notice that there's slight formatting differences between the two; the LLM received the tool output, and put it's own slight spin on it, which you may need to watch out for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dce25befa5999a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7852bf7bf753f016",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "417c73811a32f84b",
   "metadata": {},
   "source": [
    "## 6. Introduction to RAG in LangChain\n",
    "\n",
    "### Integrating Document Loaders\n",
    "\n",
    "Pre-trained language models don't have access to external data sources - their understanding comes purely from their training data. This means that if we require our model to have knowledge that goes beyond its training data, which could be company data or knowledge of more recent world events, we need a way of integrating that data. In RAG, a user query is embedded and used to retrieve the most relevant documents from the database. Then, these documents are added to the model's prompt so that the model has extra context to inform its response.\n",
    "\n",
    "<img src=\"rag-diagram.png\" width=500>\n",
    "\n",
    "There are three primary steps to RAG development in LangChain. The first is loading the documents into LangChain with document loaders. Next, is splitting the documents into chunks. Chunks are units of information that we can index and process individually. The last step is encoding and storing the chunks for retrieval, which could utilize a vector database if that meets the needs of the use case. \n",
    "\n",
    "LangChain document loaders are classes designed to load and configure documents for integration with AI systems. LangChain provides document loader classes for common file types such as CSV and PDFs. There are also additional loaders provided by 3rd parties for managing unique document formats, including Amazon S3 files, Jupyter notebooks, audio transcripts, and many more. We will practice loading data from three common formats: PDFs, CSVs, and HTML. LangChain has excellent documentation on all of its document loaders, and there's a lot of overlap in syntax, so explore at your leisure! https://python.langchain.com/docs/integrations/document_loaders\n",
    "\n",
    "There are a few different types of PDF loaders in LangChain, and there is documentation available online for each. In this tutorial, we'll use the `PyPDFLoader`. We instantiate the `PyPDFLoader` class, passing in the path to the PDF file we're loading. Finally, we use the `.load()` method to load the document into memory, and assign the resulting object to the data variable. We can then check the output to confirm that we have loaded it. Note that this document loader requires installation of the `pypdf` package as a dependency. So, install it using `pip install pypdf` or `conda install pypdf` in Termina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18223acfc7c6521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1137d388c67f6cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca30c45441e53069",
   "metadata": {},
   "source": [
    "When loading CSVs, the syntax is very similar, but instead we use the CSVLoader class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91f50d86f25cbd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b6a1ac9075593de",
   "metadata": {},
   "source": [
    "Finally, we can load HTML files using the UnstructuredHTMLLoader class. We can access the document's contents, again, with subsetting, and extract the document's metadata with the metadata attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f12f3ace1338eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9081e51b81ed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186bba71d5f8ce1",
   "metadata": {},
   "source": [
    "## Splitting external data for retrieval\n",
    "\n",
    "Now that we've loaded documents from different sources, let's learn how to parse the information. Document splitting splits the loaded document into smaller parts, which are also called chunks. Chunking is particularly useful for breaking up long documents so that they fit within an LLM's context window.\n",
    "\n",
    "Let's examine the introduction from an academic paper, which is saved as a PDF. One naive splitting option would be to separate the document by-line. This would be simple to implement, but because sentences are often split over multiple lines, and because those lines are processed separately, key context might be lost.\n",
    "\n",
    "To counteract lost context during chunk splitting, a chunk overlap is often implemented. We've selected two chunks and a chunk overlap shown in green. Having this extra overlap present in both chunks helps retain context. If a model shows signs of losing context and misunderstanding information when answering from external sources, we may need to increase this chunk overlap.\n",
    "\n",
    "<img src=\"chank-overlap.webp\" width=500>\n",
    "\n",
    "There isn't one document splitting strategy that works for all situations. We should experiment with multiple methods, and see which one strikes the right balance between retaining context and managing chunk size. We will compare two document splitting methods: `CharacterTextSplitter` and `RecursiveCharacterTextSplitter`. Optimizing this document splitting is an active area of research, so keep an eye out for new developments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4fd50c0209daed",
   "metadata": {},
   "source": [
    "As an example, let's split this quote by Elbert Hubbard, which contains 108 characters, into chunks. We'll compare how the two methods perform on this quote with a chunk_size of 24 characters and a small chunk_overlap of three.\n",
    "\n",
    "Let's start with `CharacterTextSplitter`. This method splits based on the separator first, then evaluates `chunk_size` and `chunk_overlap` to check if it's satisfied. We call `CharacterTextSplitter`, passing the `separator` to split on, along with the `chunk_size` and `chunk_overlap`. Applying the splitter to the quote with the `.split_text()` method, and printing the output, we can see that we have a problem: each of these chunks contains more characters than our specified chunk_size. `CharacterTextSplitter` splits on the separator in an attempt to make chunks smaller than `chunk_size`, but in this case, splitting on the separator was unable to return chunks below our chunk_size. Let's take a look at a more robust splitting method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef7f347e69e1dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = '''One machine can do the work of fifth ordinary humans.\\nNo machine can do the work of one extraordinary human.'''\n",
    "\n",
    "len(quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730fd7c9cdeb6342",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 24\n",
    "chunk_overlap = 3\n",
    "\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e4ab026e7f93f5",
   "metadata": {},
   "source": [
    "RecursiveCharacterSplitter takes a list of separators to split on, and it works through the list from left to right, splitting the document using each separator in turn, and seeing if these chunks can be combined while remaining under chunk_size. Let's split the quote using the same chunk_size and chunk_overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d93b085326e80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "rc_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "docs = rc_splitter.split_text(quote)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad6a24d6f8fbb4f",
   "metadata": {},
   "source": [
    "Notice how the length of each chunk varies. The class split by paragraphs first, and found that the chunk size was too big; likewise for sentences. It got to the third separator: splitting words using the space separator, and found that words could be combined into chunks while remaining under the chunk_size character limit. However, some of these chunks are too small to contain meaningful context, but this recursive implementation may work better on larger documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb76dcfecf2bad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 24\n",
    "chunk_overlap = 3\n",
    "\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = UnstructuredHTMLLoader(file_path=\"metu-regulations.html\")\n",
    "data = loader.load()\n",
    "\n",
    "rc_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separators=[\".\"],\n",
    ")\n",
    "\n",
    "docs = rc_splitter.split_documents(data)\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9133b258b42e49c8",
   "metadata": {},
   "source": [
    "We can also use split other file formats, like HTML. Recall that we can load HTML using `UnstructuredHTMLLoader`. Defining the splitter is the same, but for splitting documents, we use the `.split_documents()` method instead of `.split_text()` to perform the split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4dea659050875f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
