{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T16:22:34.610342Z",
     "start_time": "2024-10-19T16:22:34.597629Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dde0ad4459c565a",
   "metadata": {},
   "source": [
    "# A simple neuron network example with regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6d98ad8f264d302",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T16:22:34.625708Z",
     "start_time": "2024-10-19T16:22:34.618418Z"
    }
   },
   "outputs": [],
   "source": [
    "class LinearRegression(torch.nn.Module):\n",
    "    #__init__() is a special method (constructor) that Python calls when we create an object of the class.\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # super() refers to the parent class of LinearRegression. In this case, the parent is torch.nn.Module, a fundamental PyTorch class for building neural networks.\n",
    "        #By calling super().__init__(), we're ensuring that any necessary initialization from the parent class (torch.nn.Module) is properly executed before we add our own specific components to our LinearRegression model.\n",
    "        \n",
    "        \n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "        # self.linear creates an attribute named linear within our LinearRegression object. This attribute will hold our linear layer.\n",
    "        #torch.nn.Linear(input_dim, output_dim) creates a PyTorch linear layer. This layer takes an input of input_dim dimensions and transforms it into an output of output_dim dimensions. It does this using a weight matrix and a bias vector, which are learned during training.\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return self.linear(x) # This line applies the linear transformation stored in the self.linear attribute to the input x\n",
    "        # Remember that self.linear is a torch.nn.Linear object, which represents a fully connected layer with weights and biases.\n",
    "        # This operation performs a matrix multiplication of the input x with the weight matrix of the linear layer and then adds the bias vector. This is the core computation of a linear regression model. \n",
    "        # The method then returns the result of this linear transformation as the output of the model.\n",
    "    \n",
    "        # In essence, this forward() method does the following:\n",
    "\n",
    "        # 1. Takes an input x.\n",
    "        # 2. Applies a linear transformation to x using the weights and biases stored in self.linear.\n",
    "        # 3. Returns the result of the linear transformation as the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e14b80c67f22c167",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T16:22:34.704638Z",
     "start_time": "2024-10-19T16:22:34.691303Z"
    }
   },
   "outputs": [],
   "source": [
    "#Generate sample data\n",
    "X = torch.randn(100, 1) * 10\n",
    "\n",
    "y = 2 * X + 1 + torch.randn(100, 1)\n",
    "\n",
    "#torch.randn(100, 1) creates a PyTorch tensor with the shape 100x1. It's filled with random numbers sampled from a standard normal distribution (mean=0, standard deviation=1).  Essentially, you get a column vector with 100 random numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0412bd1cbd0d638",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T16:22:34.783991Z",
     "start_time": "2024-10-19T16:22:34.769980Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the model, loss function, and optimizer\n",
    "model = LinearRegression(1, 1)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98f91eaa3adee365",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T16:22:35.087659Z",
     "start_time": "2024-10-19T16:22:34.848752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 2.1888\n",
      "Epoch [200/1000], Loss: 2.1794\n",
      "Epoch [300/1000], Loss: 2.1701\n",
      "Epoch [400/1000], Loss: 2.1609\n",
      "Epoch [500/1000], Loss: 2.1517\n",
      "Epoch [600/1000], Loss: 2.1426\n",
      "Epoch [700/1000], Loss: 2.1335\n",
      "Epoch [800/1000], Loss: 2.1246\n",
      "Epoch [900/1000], Loss: 2.1156\n",
      "Epoch [1000/1000], Loss: 2.1068\n",
      "Weight: -1.4160994291305542\n",
      "Bias: 0.15423449873924255\n"
     ]
    }
   ],
   "source": [
    "#Training loop\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    #Forward pass\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    \n",
    "    #Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    #If you don't reset the gradients to zero before each backward pass, the gradients from the previous iteration will be added to the gradients of the current iteration. This leads to incorrect gradient values and ultimately disrupts the optimization process.\n",
    "    #optimizer.zero_grad() sets the gradients of all the model parameters to zero. By doing this before each loss.backward(), you ensure that the gradients calculated are solely based on the current batch of data. This prevents the unwanted accumulation of gradients from previous iterations.\n",
    "    #Imagine you're walking down a hill. The gradient represents the slope of the hill at your current location. If you keep adding the previous slope to your current slope, you'll end up with an inaccurate sense of the direction you should move in. Resetting the slope to zero each time allows you to accurately assess the current direction of descent.\n",
    "    \n",
    "    loss.backward()\n",
    "    #loss.backward() figures out the direction and magnitude of the parameter updates needed to reduce the loss.\n",
    "    #When you call loss.backward(), PyTorch performs automatic differentiation (autograd). This process traces the computations that led to the loss value and calculates the gradients of that loss with respect to every parameter in your model that requires gradients (those with requires_grad=True).\n",
    "    #These gradients are stored in the .grad attribute of each parameter. Think of it like attaching a little note to each parameter saying, \"This is how much you need to change to reduce the loss.\"\n",
    "    \n",
    "    \n",
    "    optimizer.step()\n",
    "    #optimizer.step() actually applies those updates to the model's parameters based on the chosen optimization strategy.\n",
    "    #When you call optimizer.step(), the optimizer uses the gradients (calculated by loss.backward()) to update the values of the model's parameters.\n",
    "    #The specific way it updates them depends on the optimization algorithm (SGD, Adam, etc.). SGD, for example, subtracts a fraction of the gradient (determined by the learning rate) from each parameter.\n",
    "    \n",
    "    \n",
    "    if (epoch+1) %100 == 0:\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, loss.item()))\n",
    "\n",
    "# Print the learned parameters\n",
    "print(\"Weight:\", model.linear.weight.item())\n",
    "print(\"Bias:\", model.linear.bias.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2040b16bd606932f",
   "metadata": {},
   "source": [
    "# Dropout prediction model example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262cdaed12a9a0ac",
   "metadata": {},
   "source": [
    "#### This is an example scenario where we will use a simple neural network to predict if students will drop out or not, based on their engagement in a course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a31d802514dc42df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T16:22:35.166272Z",
     "start_time": "2024-10-19T16:22:35.152354Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbe876cc02ec633",
   "metadata": {},
   "source": [
    "### Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e5f16e7b248f1c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T16:22:35.243836Z",
     "start_time": "2024-10-19T16:22:35.230825Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1258, -1.1524, -0.2506],\n",
       "        [-0.4339,  0.8487,  0.6920],\n",
       "        [-0.3160, -2.1152,  0.3223],\n",
       "        ...,\n",
       "        [ 1.2965, -0.1078,  0.7482],\n",
       "        [-0.7423,  0.3447,  1.6422],\n",
       "        [-0.3266,  0.3669, -0.7245]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate synthetic data for student dropout prediction\n",
    "\n",
    "torch.manual_seed(0) # For reproducibility\n",
    "\n",
    "num_students = 1000 # we will generate data for 1000 students.\n",
    "\n",
    "# Assume three indicators (e.g., attendances, grades, engagement)\n",
    "features = torch.randn(num_students, 3)\n",
    "\n",
    "#features is a tensor of shape (1000, 3) representing the 3 indicators for each of the 1000 students.\n",
    "features # assume that 1st column: attendances, 2nd col: grades, 3rd col: engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "773e0bc2d714de50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T16:22:35.384512Z",
     "start_time": "2024-10-19T16:22:35.355017Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3934],\n",
       "        [0.8380],\n",
       "        [0.6049],\n",
       "        [0.5903],\n",
       "        [0.9330],\n",
       "        [0.3462],\n",
       "        [0.9281],\n",
       "        [0.7110],\n",
       "        [0.8458],\n",
       "        [0.9561],\n",
       "        [0.8171],\n",
       "        [0.7878],\n",
       "        [0.9014],\n",
       "        [0.9040],\n",
       "        [0.7990],\n",
       "        [0.8750],\n",
       "        [0.8411],\n",
       "        [0.7515],\n",
       "        [0.7980],\n",
       "        [0.9930],\n",
       "        [0.6402],\n",
       "        [0.7630],\n",
       "        [0.8622],\n",
       "        [0.9804],\n",
       "        [0.2938],\n",
       "        [0.7643],\n",
       "        [0.7979],\n",
       "        [0.8414],\n",
       "        [0.9374],\n",
       "        [0.9593],\n",
       "        [0.2072],\n",
       "        [0.6979],\n",
       "        [0.9356],\n",
       "        [0.4188],\n",
       "        [0.7911],\n",
       "        [0.8584],\n",
       "        [0.8779],\n",
       "        [0.1949],\n",
       "        [0.8667],\n",
       "        [0.7791],\n",
       "        [0.8624],\n",
       "        [0.8508],\n",
       "        [0.8906],\n",
       "        [0.7249],\n",
       "        [0.9685],\n",
       "        [0.5687],\n",
       "        [0.5421],\n",
       "        [0.5770],\n",
       "        [0.7755],\n",
       "        [0.9440],\n",
       "        [0.7642],\n",
       "        [0.4786],\n",
       "        [0.6913],\n",
       "        [0.7910],\n",
       "        [0.9042],\n",
       "        [0.8169],\n",
       "        [0.8311],\n",
       "        [0.6856],\n",
       "        [0.7257],\n",
       "        [0.5281],\n",
       "        [0.9710],\n",
       "        [0.7635],\n",
       "        [0.9651],\n",
       "        [0.2740],\n",
       "        [0.7693],\n",
       "        [0.9132],\n",
       "        [0.7889],\n",
       "        [0.6795],\n",
       "        [0.8736],\n",
       "        [0.7743],\n",
       "        [0.8429],\n",
       "        [0.7680],\n",
       "        [0.9040],\n",
       "        [0.8735],\n",
       "        [0.4859],\n",
       "        [0.9605],\n",
       "        [0.7644],\n",
       "        [0.3852],\n",
       "        [0.6123],\n",
       "        [0.7883],\n",
       "        [0.3152],\n",
       "        [0.6580],\n",
       "        [0.8527],\n",
       "        [0.9106],\n",
       "        [0.8576],\n",
       "        [0.7507],\n",
       "        [0.7441],\n",
       "        [0.4856],\n",
       "        [0.8932],\n",
       "        [0.4365],\n",
       "        [0.9726],\n",
       "        [0.9161],\n",
       "        [0.2786],\n",
       "        [0.7741],\n",
       "        [0.6912],\n",
       "        [0.6251],\n",
       "        [0.8811],\n",
       "        [0.4605],\n",
       "        [0.6459],\n",
       "        [0.9415],\n",
       "        [0.9613],\n",
       "        [0.6169],\n",
       "        [0.7611],\n",
       "        [0.3689],\n",
       "        [0.3554],\n",
       "        [0.6117],\n",
       "        [0.6647],\n",
       "        [0.8971],\n",
       "        [0.9864],\n",
       "        [0.7853],\n",
       "        [0.8471],\n",
       "        [0.2199],\n",
       "        [0.9634],\n",
       "        [0.7919],\n",
       "        [0.8946],\n",
       "        [0.0911],\n",
       "        [0.9647],\n",
       "        [0.8104],\n",
       "        [0.9481],\n",
       "        [0.9817],\n",
       "        [0.9383],\n",
       "        [0.4431],\n",
       "        [0.8123],\n",
       "        [0.9653],\n",
       "        [0.8245],\n",
       "        [0.8081],\n",
       "        [0.5545],\n",
       "        [0.8334],\n",
       "        [0.9400],\n",
       "        [0.7936],\n",
       "        [0.9468],\n",
       "        [0.9910],\n",
       "        [0.8402],\n",
       "        [0.6308],\n",
       "        [0.8334],\n",
       "        [0.6638],\n",
       "        [0.9553],\n",
       "        [0.8982],\n",
       "        [0.9842],\n",
       "        [0.9574],\n",
       "        [0.8067],\n",
       "        [0.6192],\n",
       "        [0.9695],\n",
       "        [0.4985],\n",
       "        [0.9746],\n",
       "        [0.9264],\n",
       "        [0.7104],\n",
       "        [0.7311],\n",
       "        [0.9259],\n",
       "        [0.6429],\n",
       "        [0.9938],\n",
       "        [0.8533],\n",
       "        [0.8663],\n",
       "        [0.9781],\n",
       "        [0.8388],\n",
       "        [0.4719],\n",
       "        [0.8720],\n",
       "        [0.7297],\n",
       "        [0.5725],\n",
       "        [0.2205],\n",
       "        [0.5833],\n",
       "        [0.6432],\n",
       "        [0.9043],\n",
       "        [0.8228],\n",
       "        [0.5476],\n",
       "        [0.8869],\n",
       "        [0.4033],\n",
       "        [0.5967],\n",
       "        [0.4902],\n",
       "        [0.0973],\n",
       "        [0.8334],\n",
       "        [0.7512],\n",
       "        [0.9369],\n",
       "        [0.6671],\n",
       "        [0.7248],\n",
       "        [0.9501],\n",
       "        [0.3712],\n",
       "        [0.1960],\n",
       "        [0.8134],\n",
       "        [0.8847],\n",
       "        [0.8728],\n",
       "        [0.9721],\n",
       "        [0.4727],\n",
       "        [0.2397],\n",
       "        [0.8485],\n",
       "        [0.9602],\n",
       "        [0.7605],\n",
       "        [0.8658],\n",
       "        [0.6744],\n",
       "        [0.8398],\n",
       "        [0.6551],\n",
       "        [0.9652],\n",
       "        [0.3863],\n",
       "        [0.6685],\n",
       "        [0.8366],\n",
       "        [0.3350],\n",
       "        [0.6726],\n",
       "        [0.7704],\n",
       "        [0.9363],\n",
       "        [0.6919],\n",
       "        [0.9623],\n",
       "        [0.9493],\n",
       "        [0.3759],\n",
       "        [0.5836],\n",
       "        [0.6324],\n",
       "        [0.9528],\n",
       "        [0.9058],\n",
       "        [0.7117],\n",
       "        [0.4817],\n",
       "        [0.7807],\n",
       "        [0.8743],\n",
       "        [0.6960],\n",
       "        [0.9275],\n",
       "        [0.3642],\n",
       "        [0.8507],\n",
       "        [0.9431],\n",
       "        [0.7312],\n",
       "        [0.9814],\n",
       "        [0.8231],\n",
       "        [0.9870],\n",
       "        [0.9190],\n",
       "        [0.5890],\n",
       "        [0.8598],\n",
       "        [0.7185],\n",
       "        [0.8162],\n",
       "        [0.4042],\n",
       "        [0.4069],\n",
       "        [0.9522],\n",
       "        [0.6319],\n",
       "        [0.9441],\n",
       "        [0.4447],\n",
       "        [0.5849],\n",
       "        [0.4051],\n",
       "        [0.4999],\n",
       "        [0.8848],\n",
       "        [0.8423],\n",
       "        [0.6162],\n",
       "        [0.9064],\n",
       "        [0.8445],\n",
       "        [0.3367],\n",
       "        [0.8031],\n",
       "        [0.6690],\n",
       "        [0.8243],\n",
       "        [0.8513],\n",
       "        [0.9564],\n",
       "        [0.7994],\n",
       "        [0.8458],\n",
       "        [0.6422],\n",
       "        [0.3234],\n",
       "        [0.9319],\n",
       "        [0.7776],\n",
       "        [0.9655],\n",
       "        [0.7160],\n",
       "        [0.4925],\n",
       "        [0.7319],\n",
       "        [0.9511],\n",
       "        [0.9484],\n",
       "        [0.9337],\n",
       "        [0.3387],\n",
       "        [0.9011],\n",
       "        [0.7067],\n",
       "        [0.7387],\n",
       "        [0.7774],\n",
       "        [0.6898],\n",
       "        [0.8240],\n",
       "        [0.9313],\n",
       "        [0.7670],\n",
       "        [0.5646],\n",
       "        [0.2579],\n",
       "        [0.6514],\n",
       "        [0.9516],\n",
       "        [0.8999],\n",
       "        [0.2781],\n",
       "        [0.3023],\n",
       "        [0.8781],\n",
       "        [0.9696],\n",
       "        [0.1151],\n",
       "        [0.8503],\n",
       "        [0.6197],\n",
       "        [0.7827],\n",
       "        [0.8368],\n",
       "        [0.4489],\n",
       "        [0.3070],\n",
       "        [0.9338],\n",
       "        [0.7168],\n",
       "        [0.8629],\n",
       "        [0.4587],\n",
       "        [0.7253],\n",
       "        [0.9655],\n",
       "        [0.8583],\n",
       "        [0.7853],\n",
       "        [0.7179],\n",
       "        [0.8013],\n",
       "        [0.8000],\n",
       "        [0.8098],\n",
       "        [0.8934],\n",
       "        [0.7960],\n",
       "        [0.2608],\n",
       "        [0.9503],\n",
       "        [0.5663],\n",
       "        [0.3518],\n",
       "        [0.6544],\n",
       "        [0.9633],\n",
       "        [0.9049],\n",
       "        [0.9659],\n",
       "        [0.9029],\n",
       "        [0.5353],\n",
       "        [0.6243],\n",
       "        [0.4967],\n",
       "        [0.9669],\n",
       "        [0.9646],\n",
       "        [0.6959],\n",
       "        [0.1237],\n",
       "        [0.9196],\n",
       "        [0.9761],\n",
       "        [0.8966],\n",
       "        [0.4240],\n",
       "        [0.7394],\n",
       "        [0.6326],\n",
       "        [0.8594],\n",
       "        [0.5423],\n",
       "        [0.8355],\n",
       "        [0.7696],\n",
       "        [0.3508],\n",
       "        [0.9062],\n",
       "        [0.7872],\n",
       "        [0.6018],\n",
       "        [0.7737],\n",
       "        [0.7973],\n",
       "        [0.8740],\n",
       "        [0.4624],\n",
       "        [0.8836],\n",
       "        [0.6889],\n",
       "        [0.9128],\n",
       "        [0.8942],\n",
       "        [0.7959],\n",
       "        [0.8873],\n",
       "        [0.6800],\n",
       "        [0.7632],\n",
       "        [0.7763],\n",
       "        [0.6464],\n",
       "        [0.2875],\n",
       "        [0.6433],\n",
       "        [0.7026],\n",
       "        [0.8467],\n",
       "        [0.8295],\n",
       "        [0.9130],\n",
       "        [0.8722],\n",
       "        [0.9715],\n",
       "        [0.1426],\n",
       "        [0.7363],\n",
       "        [0.7052],\n",
       "        [0.3772],\n",
       "        [0.9596],\n",
       "        [0.6584],\n",
       "        [0.9561],\n",
       "        [0.3206],\n",
       "        [0.9021],\n",
       "        [0.4942],\n",
       "        [0.7037],\n",
       "        [0.1188],\n",
       "        [0.6670],\n",
       "        [0.1801],\n",
       "        [0.5867],\n",
       "        [0.5998],\n",
       "        [0.8851],\n",
       "        [0.8440],\n",
       "        [0.8407],\n",
       "        [0.7522],\n",
       "        [0.8481],\n",
       "        [0.9151],\n",
       "        [0.4859],\n",
       "        [0.7795],\n",
       "        [0.8172],\n",
       "        [0.8430],\n",
       "        [0.8938],\n",
       "        [0.6604],\n",
       "        [0.4575],\n",
       "        [0.8721],\n",
       "        [0.8741],\n",
       "        [0.8779],\n",
       "        [0.8500],\n",
       "        [0.2938],\n",
       "        [0.8621],\n",
       "        [0.8159],\n",
       "        [0.9333],\n",
       "        [0.4610],\n",
       "        [0.9489],\n",
       "        [0.6908],\n",
       "        [0.6594],\n",
       "        [0.6358],\n",
       "        [0.9022],\n",
       "        [0.7434],\n",
       "        [0.9773],\n",
       "        [0.6722],\n",
       "        [0.8850],\n",
       "        [0.9178],\n",
       "        [0.4073],\n",
       "        [0.6435],\n",
       "        [0.8869],\n",
       "        [0.4735],\n",
       "        [0.8590],\n",
       "        [0.9616],\n",
       "        [0.9895],\n",
       "        [0.7137],\n",
       "        [0.7067],\n",
       "        [0.8723],\n",
       "        [0.9469],\n",
       "        [0.8305],\n",
       "        [0.5074],\n",
       "        [0.7731],\n",
       "        [0.7693],\n",
       "        [0.9456],\n",
       "        [0.6760],\n",
       "        [0.7069],\n",
       "        [0.9187],\n",
       "        [0.9775],\n",
       "        [0.9806],\n",
       "        [0.8950],\n",
       "        [0.4278],\n",
       "        [0.7466],\n",
       "        [0.9571],\n",
       "        [0.7162],\n",
       "        [0.8901],\n",
       "        [0.9421],\n",
       "        [0.9044],\n",
       "        [0.3524],\n",
       "        [0.9356],\n",
       "        [0.6334],\n",
       "        [0.9818],\n",
       "        [0.6733],\n",
       "        [0.9428],\n",
       "        [0.8635],\n",
       "        [0.7525],\n",
       "        [0.8027],\n",
       "        [0.8175],\n",
       "        [0.8104],\n",
       "        [0.8552],\n",
       "        [0.6146],\n",
       "        [0.6201],\n",
       "        [0.8664],\n",
       "        [0.9199],\n",
       "        [0.9152],\n",
       "        [0.6455],\n",
       "        [0.7638],\n",
       "        [0.9024],\n",
       "        [0.8579],\n",
       "        [0.4718],\n",
       "        [0.7129],\n",
       "        [0.6628],\n",
       "        [0.4941],\n",
       "        [0.7746],\n",
       "        [0.9471],\n",
       "        [0.8325],\n",
       "        [0.9377],\n",
       "        [0.2178],\n",
       "        [0.9004],\n",
       "        [0.6076],\n",
       "        [0.8670],\n",
       "        [0.8530],\n",
       "        [0.8617],\n",
       "        [0.8197],\n",
       "        [0.9809],\n",
       "        [0.6856],\n",
       "        [0.8150],\n",
       "        [0.5254],\n",
       "        [0.8512],\n",
       "        [0.9560],\n",
       "        [0.3230],\n",
       "        [0.9526],\n",
       "        [0.4971],\n",
       "        [0.9469],\n",
       "        [0.6805],\n",
       "        [0.8549],\n",
       "        [0.9830],\n",
       "        [0.7551],\n",
       "        [0.7174],\n",
       "        [0.9640],\n",
       "        [0.9800],\n",
       "        [0.7941],\n",
       "        [0.7886],\n",
       "        [0.7788],\n",
       "        [0.8532],\n",
       "        [0.7104],\n",
       "        [0.5718],\n",
       "        [0.8615],\n",
       "        [0.8929],\n",
       "        [0.5684],\n",
       "        [0.7926],\n",
       "        [0.5795],\n",
       "        [0.9755],\n",
       "        [0.9452],\n",
       "        [0.8181],\n",
       "        [0.5184],\n",
       "        [0.7972],\n",
       "        [0.7427],\n",
       "        [0.7836],\n",
       "        [0.6035],\n",
       "        [0.8328],\n",
       "        [0.8411],\n",
       "        [0.4898],\n",
       "        [0.9757],\n",
       "        [0.6842],\n",
       "        [0.2180],\n",
       "        [0.9349],\n",
       "        [0.8597],\n",
       "        [0.6569],\n",
       "        [0.5845],\n",
       "        [0.8666],\n",
       "        [0.9453],\n",
       "        [0.5996],\n",
       "        [0.8762],\n",
       "        [0.4575],\n",
       "        [0.8897],\n",
       "        [0.6553],\n",
       "        [0.7185],\n",
       "        [0.9922],\n",
       "        [0.1045],\n",
       "        [0.8320],\n",
       "        [0.8114],\n",
       "        [0.4627],\n",
       "        [0.8040],\n",
       "        [0.8184],\n",
       "        [0.8576],\n",
       "        [0.3771],\n",
       "        [0.8656],\n",
       "        [0.7830],\n",
       "        [0.6077],\n",
       "        [0.6751],\n",
       "        [0.3965],\n",
       "        [0.8805],\n",
       "        [0.8175],\n",
       "        [0.7942],\n",
       "        [0.8002],\n",
       "        [0.2521],\n",
       "        [0.6104],\n",
       "        [0.8093],\n",
       "        [0.7197],\n",
       "        [0.9279],\n",
       "        [0.7826],\n",
       "        [0.7845],\n",
       "        [0.0942],\n",
       "        [0.4208],\n",
       "        [0.8570],\n",
       "        [0.9562],\n",
       "        [0.6777],\n",
       "        [0.4150],\n",
       "        [0.8580],\n",
       "        [0.8534],\n",
       "        [0.4573],\n",
       "        [0.7586],\n",
       "        [0.8357],\n",
       "        [0.5560],\n",
       "        [0.7402],\n",
       "        [0.4336],\n",
       "        [0.1210],\n",
       "        [0.9840],\n",
       "        [0.8209],\n",
       "        [0.7828],\n",
       "        [0.9776],\n",
       "        [0.8551],\n",
       "        [0.6745],\n",
       "        [0.9305],\n",
       "        [0.2450],\n",
       "        [0.5252],\n",
       "        [0.9097],\n",
       "        [0.6856],\n",
       "        [0.7676],\n",
       "        [0.9543],\n",
       "        [0.8962],\n",
       "        [0.6041],\n",
       "        [0.4565],\n",
       "        [0.5942],\n",
       "        [0.8586],\n",
       "        [0.9691],\n",
       "        [0.6070],\n",
       "        [0.7437],\n",
       "        [0.9612],\n",
       "        [0.1722],\n",
       "        [0.4923],\n",
       "        [0.9182],\n",
       "        [0.8673],\n",
       "        [0.4808],\n",
       "        [0.6781],\n",
       "        [0.7347],\n",
       "        [0.9055],\n",
       "        [0.7680],\n",
       "        [0.7526],\n",
       "        [0.3672],\n",
       "        [0.9877],\n",
       "        [0.9701],\n",
       "        [0.4728],\n",
       "        [0.6894],\n",
       "        [0.9868],\n",
       "        [0.4618],\n",
       "        [0.9650],\n",
       "        [0.7253],\n",
       "        [0.2560],\n",
       "        [0.9445],\n",
       "        [0.7219],\n",
       "        [0.7564],\n",
       "        [0.4897],\n",
       "        [0.9708],\n",
       "        [0.5905],\n",
       "        [0.1956],\n",
       "        [0.3749],\n",
       "        [0.8853],\n",
       "        [0.8350],\n",
       "        [0.5789],\n",
       "        [0.9845],\n",
       "        [0.7661],\n",
       "        [0.9126],\n",
       "        [0.7049],\n",
       "        [0.7843],\n",
       "        [0.6602],\n",
       "        [0.8015],\n",
       "        [0.7024],\n",
       "        [0.9126],\n",
       "        [0.3749],\n",
       "        [0.5279],\n",
       "        [0.9465],\n",
       "        [0.7882],\n",
       "        [0.4690],\n",
       "        [0.4026],\n",
       "        [0.9486],\n",
       "        [0.8711],\n",
       "        [0.6756],\n",
       "        [0.9067],\n",
       "        [0.4892],\n",
       "        [0.8159],\n",
       "        [0.6219],\n",
       "        [0.4867],\n",
       "        [0.7667],\n",
       "        [0.0578],\n",
       "        [0.8953],\n",
       "        [0.8750],\n",
       "        [0.5940],\n",
       "        [0.8567],\n",
       "        [0.5656],\n",
       "        [0.3158],\n",
       "        [0.7071],\n",
       "        [0.9409],\n",
       "        [0.7846],\n",
       "        [0.8644],\n",
       "        [0.3911],\n",
       "        [0.5515],\n",
       "        [0.8107],\n",
       "        [0.7315],\n",
       "        [0.7835],\n",
       "        [0.8707],\n",
       "        [0.9379],\n",
       "        [0.7172],\n",
       "        [0.6559],\n",
       "        [0.7397],\n",
       "        [0.9558],\n",
       "        [0.9782],\n",
       "        [0.7854],\n",
       "        [0.8598],\n",
       "        [0.4975],\n",
       "        [0.8523],\n",
       "        [0.8641],\n",
       "        [0.5815],\n",
       "        [0.6303],\n",
       "        [0.2228],\n",
       "        [0.7474],\n",
       "        [0.6489],\n",
       "        [0.7543],\n",
       "        [0.9377],\n",
       "        [0.9383],\n",
       "        [0.5611],\n",
       "        [0.9404],\n",
       "        [0.4619],\n",
       "        [0.5288],\n",
       "        [0.7843],\n",
       "        [0.5603],\n",
       "        [0.8016],\n",
       "        [0.8778],\n",
       "        [0.5375],\n",
       "        [0.9165],\n",
       "        [0.6857],\n",
       "        [0.7955],\n",
       "        [0.5904],\n",
       "        [0.6171],\n",
       "        [0.8687],\n",
       "        [0.5087],\n",
       "        [0.5333],\n",
       "        [0.5864],\n",
       "        [0.5114],\n",
       "        [0.9482],\n",
       "        [0.7510],\n",
       "        [0.6971],\n",
       "        [0.3176],\n",
       "        [0.6610],\n",
       "        [0.9538],\n",
       "        [0.9522],\n",
       "        [0.7451],\n",
       "        [0.9249],\n",
       "        [0.9373],\n",
       "        [0.9390],\n",
       "        [0.8338],\n",
       "        [0.7019],\n",
       "        [0.4292],\n",
       "        [0.8642],\n",
       "        [0.6058],\n",
       "        [0.5555],\n",
       "        [0.5599],\n",
       "        [0.9638],\n",
       "        [0.3949],\n",
       "        [0.6378],\n",
       "        [0.7402],\n",
       "        [0.8947],\n",
       "        [0.9698],\n",
       "        [0.4736],\n",
       "        [0.5017],\n",
       "        [0.8839],\n",
       "        [0.6017],\n",
       "        [0.7727],\n",
       "        [0.3211],\n",
       "        [0.6029],\n",
       "        [0.9564],\n",
       "        [0.4836],\n",
       "        [0.7145],\n",
       "        [0.6266],\n",
       "        [0.7908],\n",
       "        [0.8212],\n",
       "        [0.6635],\n",
       "        [0.4206],\n",
       "        [0.5270],\n",
       "        [0.9303],\n",
       "        [0.6689],\n",
       "        [0.9477],\n",
       "        [0.8539],\n",
       "        [0.9592],\n",
       "        [0.6812],\n",
       "        [0.5921],\n",
       "        [0.7226],\n",
       "        [0.8913],\n",
       "        [0.9079],\n",
       "        [0.1796],\n",
       "        [0.9232],\n",
       "        [0.7388],\n",
       "        [0.8294],\n",
       "        [0.9967],\n",
       "        [0.7511],\n",
       "        [0.5788],\n",
       "        [0.5584],\n",
       "        [0.5412],\n",
       "        [0.8549],\n",
       "        [0.8790],\n",
       "        [0.6101],\n",
       "        [0.7101],\n",
       "        [0.5323],\n",
       "        [0.7834],\n",
       "        [0.8533],\n",
       "        [0.8655],\n",
       "        [0.4943],\n",
       "        [0.7521],\n",
       "        [0.2444],\n",
       "        [0.2899],\n",
       "        [0.8760],\n",
       "        [0.6136],\n",
       "        [0.8544],\n",
       "        [0.6656],\n",
       "        [0.8819],\n",
       "        [0.7200],\n",
       "        [0.6976],\n",
       "        [0.8049],\n",
       "        [0.8106],\n",
       "        [0.7198],\n",
       "        [0.6112],\n",
       "        [0.5411],\n",
       "        [0.7114],\n",
       "        [0.7754],\n",
       "        [0.6709],\n",
       "        [0.4765],\n",
       "        [0.7607],\n",
       "        [0.3401],\n",
       "        [0.9383],\n",
       "        [0.4926],\n",
       "        [0.9694],\n",
       "        [0.8187],\n",
       "        [0.1602],\n",
       "        [0.5967],\n",
       "        [0.7026],\n",
       "        [0.6578],\n",
       "        [0.8098],\n",
       "        [0.7048],\n",
       "        [0.8128],\n",
       "        [0.9765],\n",
       "        [0.7037],\n",
       "        [0.9644],\n",
       "        [0.5906],\n",
       "        [0.5958],\n",
       "        [0.3673],\n",
       "        [0.9721],\n",
       "        [0.8073],\n",
       "        [0.9056],\n",
       "        [0.8000],\n",
       "        [0.8836],\n",
       "        [0.6378],\n",
       "        [0.9303],\n",
       "        [0.9144],\n",
       "        [0.9487],\n",
       "        [0.8599],\n",
       "        [0.7654],\n",
       "        [0.6319],\n",
       "        [0.7261],\n",
       "        [0.4955],\n",
       "        [0.7240],\n",
       "        [0.9381],\n",
       "        [0.6151],\n",
       "        [0.6132],\n",
       "        [0.9515],\n",
       "        [0.2379],\n",
       "        [0.9824],\n",
       "        [0.8501],\n",
       "        [0.9202],\n",
       "        [0.3151],\n",
       "        [0.9185],\n",
       "        [0.7058],\n",
       "        [0.4203],\n",
       "        [0.8850],\n",
       "        [0.9126],\n",
       "        [0.8616],\n",
       "        [0.7839],\n",
       "        [0.9809],\n",
       "        [0.8543],\n",
       "        [0.5112],\n",
       "        [0.9077],\n",
       "        [0.6384],\n",
       "        [0.7275],\n",
       "        [0.8648],\n",
       "        [0.7211],\n",
       "        [0.9567],\n",
       "        [0.8221],\n",
       "        [0.9532],\n",
       "        [0.8952],\n",
       "        [0.1941],\n",
       "        [0.8383],\n",
       "        [0.9120],\n",
       "        [0.9018],\n",
       "        [0.8382],\n",
       "        [0.5878],\n",
       "        [0.5337],\n",
       "        [0.9803],\n",
       "        [0.6982],\n",
       "        [0.8883],\n",
       "        [0.9457],\n",
       "        [0.9118],\n",
       "        [0.8688],\n",
       "        [0.5372],\n",
       "        [0.5424],\n",
       "        [0.8165],\n",
       "        [0.9457],\n",
       "        [0.4817],\n",
       "        [0.7245],\n",
       "        [0.4644],\n",
       "        [0.5262],\n",
       "        [0.6433],\n",
       "        [0.9441],\n",
       "        [0.8429],\n",
       "        [0.2934],\n",
       "        [0.7113],\n",
       "        [0.2483],\n",
       "        [0.8463],\n",
       "        [0.6829],\n",
       "        [0.6416],\n",
       "        [0.9279],\n",
       "        [0.7949],\n",
       "        [0.9170],\n",
       "        [0.8862],\n",
       "        [0.4882],\n",
       "        [0.8567],\n",
       "        [0.5111],\n",
       "        [0.8181],\n",
       "        [0.6452],\n",
       "        [0.6173],\n",
       "        [0.7822],\n",
       "        [0.9272],\n",
       "        [0.7376],\n",
       "        [0.7740],\n",
       "        [0.9664],\n",
       "        [0.3727],\n",
       "        [0.3675],\n",
       "        [0.6005],\n",
       "        [0.8284],\n",
       "        [0.1906],\n",
       "        [0.2613],\n",
       "        [0.7161],\n",
       "        [0.8893],\n",
       "        [0.7199],\n",
       "        [0.8762],\n",
       "        [0.9680],\n",
       "        [0.6018],\n",
       "        [0.8384],\n",
       "        [0.7471],\n",
       "        [0.9118],\n",
       "        [0.6342],\n",
       "        [0.5248],\n",
       "        [0.5867],\n",
       "        [0.4052],\n",
       "        [0.8656],\n",
       "        [0.9555],\n",
       "        [0.9530],\n",
       "        [0.7748],\n",
       "        [0.9339],\n",
       "        [0.9322],\n",
       "        [0.9847],\n",
       "        [0.9156],\n",
       "        [0.5781],\n",
       "        [0.8782],\n",
       "        [0.8529],\n",
       "        [0.4159],\n",
       "        [0.5505],\n",
       "        [0.5522],\n",
       "        [0.9618],\n",
       "        [0.3724],\n",
       "        [0.8783],\n",
       "        [0.8178],\n",
       "        [0.6749],\n",
       "        [0.5173],\n",
       "        [0.9797],\n",
       "        [0.7820],\n",
       "        [0.7840],\n",
       "        [0.8921],\n",
       "        [0.3563],\n",
       "        [0.7028],\n",
       "        [0.3570],\n",
       "        [0.9240],\n",
       "        [0.9666],\n",
       "        [0.9280],\n",
       "        [0.7813],\n",
       "        [0.6400],\n",
       "        [0.8738],\n",
       "        [0.3576],\n",
       "        [0.8169],\n",
       "        [0.7142],\n",
       "        [0.6204],\n",
       "        [0.8922],\n",
       "        [0.6494],\n",
       "        [0.4992],\n",
       "        [0.5280],\n",
       "        [0.9454],\n",
       "        [0.9660],\n",
       "        [0.9285],\n",
       "        [0.4413],\n",
       "        [0.8336],\n",
       "        [0.9536],\n",
       "        [0.9470],\n",
       "        [0.6152],\n",
       "        [0.7512],\n",
       "        [0.5796],\n",
       "        [0.4662],\n",
       "        [0.3554],\n",
       "        [0.9544],\n",
       "        [0.5532],\n",
       "        [0.5200],\n",
       "        [0.8336],\n",
       "        [0.9157],\n",
       "        [0.8429],\n",
       "        [0.8588],\n",
       "        [0.8260],\n",
       "        [0.6534],\n",
       "        [0.6282],\n",
       "        [0.5987],\n",
       "        [0.3268],\n",
       "        [0.1641],\n",
       "        [0.7204],\n",
       "        [0.4920],\n",
       "        [0.9608],\n",
       "        [0.8063],\n",
       "        [0.8075],\n",
       "        [0.8012],\n",
       "        [0.8586],\n",
       "        [0.8191],\n",
       "        [0.9734],\n",
       "        [0.9629],\n",
       "        [0.7504],\n",
       "        [0.4186],\n",
       "        [0.3849],\n",
       "        [0.9204],\n",
       "        [0.8001],\n",
       "        [0.9560],\n",
       "        [0.7345],\n",
       "        [0.5134],\n",
       "        [0.7016],\n",
       "        [0.8032],\n",
       "        [0.7690],\n",
       "        [0.8118],\n",
       "        [0.5244],\n",
       "        [0.6277],\n",
       "        [0.8988],\n",
       "        [0.8899],\n",
       "        [0.9538],\n",
       "        [0.8913],\n",
       "        [0.3174],\n",
       "        [0.8052],\n",
       "        [0.9554],\n",
       "        [0.8599],\n",
       "        [0.6428]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_weights = torch.rand(3, 1)\n",
    "random_bias = torch.randn(1)\n",
    "dropout_probs = torch.sigmoid(features @ random_weights + random_bias)\n",
    "#@ is the matrix multiplication operator in PyTorch. This operation essentially performs a weighted sum of the features for each student. The result is a tensor of shape (1000, 1)\n",
    "\n",
    "# Generate probabilities\n",
    "# It first calculates a weighted sum of the features, where the weights are random.\n",
    "# It adds a random bias to this sum.\n",
    "# Finally, it uses the sigmoid function to transform the result into a probability between 0 and 1.\n",
    "\n",
    "# This approach simulates a scenario where the likelihood of a student dropping out is influenced by their attendance, grades, and engagement, but with some randomness introduced by the random weights and bias. This randomness reflects the complexity of real-world factors affecting dropout rates.\n",
    "\n",
    "dropout_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce34d0fee3c51ae6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T16:22:35.510166Z",
     "start_time": "2024-10-19T16:22:35.486652Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
       "        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
       "        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
       "        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "        1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "        1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
       "        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
       "        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
       "        1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 0., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = (dropout_probs > 0.5).float().squeeze()  # Dropout (1) or not (0)\n",
    "#squaeze  transforms the tensor from shape (1000, 1) to shape (1000). This results in a 1-dimensional tensor where each element corresponds to a student's dropout status (1.0 for dropout, 0.0 for no dropout).\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bce968ba38d104f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T16:22:35.634116Z",
     "start_time": "2024-10-19T16:22:35.617116Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split data into training, validation, and test sets\n",
    "# We have to use a separate set of data for training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d7d4d7798cdff9",
   "metadata": {},
   "source": [
    "### Defining the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0c69dc73f559185",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T16:22:35.650120Z",
     "start_time": "2024-10-19T16:22:35.646135Z"
    }
   },
   "outputs": [],
   "source": [
    "#Define the model with multiple hidden layers\n",
    "class DropoutPredictionModel(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        #!!! Define three layers fc1, fc2, and fc3. The parameter values should derived from the explanations below:\n",
    "        \n",
    "        # Input layer: 3 indicators\n",
    "        # Hidden layer 1: The values 16 and 8 are random.\n",
    "        # Output layer: dropout probability\n",
    "        #  <<Your code goes here>>\n",
    "        self.fc1 = nn.Linear(3,16)\n",
    "        self.fc2 = nn.Linear(16,8)\n",
    "        self.fc3 = nn.Linear(8,1)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x)) # ReLU introduces a non-linear transformation by setting all negative values to zero. This simple operation allows the network to approximate non-linear functions, which are essential for modeling real-world data and relationships.\n",
    "        # Vanishing gradients hinder learning: In deep networks with sigmoid or tanh activations, gradients can become very small during backpropagation. This \"vanishing gradient\" problem makes it difficult for the network to learn effectively.  \n",
    "        # ReLU helps: ReLU doesn't suffer from vanishing gradients as much as sigmoid or tanh, especially for positive values. The gradient of ReLU is 1 for positive inputs, allowing gradients to flow more easily through the network.\n",
    "        #Zeroing out negative values: ReLU introduces sparsity by setting negative activations to zero. This can make the network more computationally efficient and can also lead to better generalization by preventing overfitting.\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = torch.sigmoid(self.fc3(x)) #For binary classification problems, you need an activation function in your output layer that produces a probability-like value between 0 and 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f7551450d82b0a",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f10477c1557139ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T16:22:53.362164Z",
     "start_time": "2024-10-19T16:22:53.356126Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the model, loss function, and optimizer with momentum\n",
    "\n",
    "\n",
    "model = DropoutPredictionModel()\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(3,16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16,8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8,1),\n",
    "    nn.Dropout(p=0.3)\n",
    ")\n",
    "model.to(\"cpu\")\n",
    "criterion = nn.BCELoss() # Binary Cross Entropy Loss for binary classification\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0fafd888b939bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    # !!! Switch the model to the training mode \n",
    "    model.train()\n",
    "    # <<Your code goes here>>\n",
    "    \n",
    "    #!!! Get the predictions and store them inside outputs variable\n",
    "    outputs = model(X_train)\n",
    "    # <<Your code goes here>> \n",
    "    # Neural networks have different behaviors during training and evaluation (inference). model.train() sets the model to training mode\n",
    "    \n",
    "    #!!! Compute the loss. Remember to apply unsqueeze(1) to y_train\n",
    "    loss = criterion(outputs, y_train.unsqueeze(1))\n",
    "    # <<Your code goes here>>\n",
    "    \n",
    "    # Do the back propagation\n",
    "    loss.backward() \n",
    "    # <<Your code goes here>>\n",
    "    # Every time you process a batch of inputs with .backward(), you accumulate \"suggestions\" of where to step.\n",
    "    \n",
    "    #!!! Apply the optimization\n",
    "    # <<Your code goes here>> \n",
    "    optimizer.step()\n",
    "    # Notice that a \"suggestion\" is much weaker than a \"decision\". When you call optimizer.step(), the optimizer uses these suggestions to make actual decisions of where to actually step. The optimizer reads the suggestions and then steps in a direction that it hopes will minimize future losses.\n",
    "\n",
    "    #!!! Reset the gradients\n",
    "    # <<Your code goes here>>\n",
    "    optimizer.zero_grad() \n",
    "     # Once you've completed a step, you don't really need to keep track of your previous suggestion (i.e. gradients) of where to step. By zeroing the gradients, you are throwing away this information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b271f3edf8fb4a96",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eeb0c8102ac0ba38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T16:22:54.603909Z",
     "start_time": "2024-10-19T16:22:54.594931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8733\n"
     ]
    }
   ],
   "source": [
    "model.eval() # Switches the mode to the evaluation. \n",
    "\n",
    "with torch.no_grad(): # This code creates a context within which PyTorch will not track gradients. This ensures that no gradients are calculated during validation, saving resources and preventing unintended changes to the model's parameters.\n",
    "    \n",
    "    #!!! Obtain the predictions on the test data: X_test and store the results into test_outputs\n",
    "    # <<Your code goes here>> \n",
    "    # Produces the outputs: predictions on dropouts\n",
    "    test_outputs = model(X_test)\n",
    "    \n",
    "    test_preds = (test_outputs > 0.5).float().squeeze() # converts the probabilities into 0 and 1\n",
    "    \n",
    "    #!!! Compute the accuracy by calling accuracy_score with parameters of y_test and test_preds\n",
    "    # <<Your code goes here>> \n",
    "    # computes the accuracy\n",
    "    test_acc = accuracy_score(y_test, test_preds)\n",
    "    print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6e2476d225cad7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T16:22:36.009544Z",
     "start_time": "2024-10-19T16:22:35.992987Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
