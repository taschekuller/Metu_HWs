{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Local RAG Application using Ollama and LangChain\n",
    "\n",
    "<sub style=\"display:none\">This tutorial is preparated based on https://medium.com/@himanshushukla.shukla3/build-a-local-rag-application-42c06a9051e4</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local LLMs (Large Language Models) are AI models that run on your personal devices (like a computer or smartphone) instead of relying on cloud services. Running LLMs locally brings several key advantages:   \n",
    "\n",
    "* Your data stays with you, minimizing concerns about sensitive information being sent to and stored on external servers.   \n",
    "* You have complete control over the model and its parameters, allowing for customization and experimentation.   \n",
    "* Use the LLM anytime, anywhere, even without an internet connection.\n",
    "* Potentially avoid ongoing cloud usage fees or API call charges.   \n",
    "\n",
    "This emerging trend empowers users with greater autonomy and flexibility, opening up new possibilities for personalized AI applications while addressing growing concerns about data privacy and security.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama\n",
    "\n",
    "Ollama is a tool that simplifies the process of running LLMs locally. Think of it as a user-friendly platform specifically designed for managing and interacting with local LLMs. \n",
    "\n",
    "Essentially, Ollama lowers the barrier to entry for running LLMs locally. By simplifying the technical aspects, it allows users to focus on exploring the capabilities of these models and developing innovative applications.\n",
    "\n",
    "Here's what makes it important in the context of local LLMs:\n",
    "\n",
    "* Ollama streamlines the often complex process of installing and configuring LLMs, making them accessible to a wider audience.\n",
    "* It provides a centralized interface to download, organize, and run various LLMs, eliminating the need for manual configuration of each model.\n",
    "* Ollama helps manage system resources effectively, ensuring smooth performance even on devices with limited hardware capabilities.\n",
    "\n",
    "> To learn how to install and use Ollama, please refer to the Ollama guide in OdtuClass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing the packages\n",
    "\n",
    "Please run the following cell to install the required Python packages. Since we will use langchain, we will import the langchain implementations of Ollama and Chromadb (a vector database).\n",
    "\n",
    "> Installation may take a while depending on the compute power of your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document loading, retrieval methods and text splitting\n",
    "!pip install -qU langchain langchain_community\n",
    "\n",
    "# Local vector store via Chroma\n",
    "!pip install -qU langchain_chroma\n",
    "\n",
    "# Local inference and embeddings via Ollama\n",
    "!pip install -qU langchain_ollama\n",
    "\n",
    "# Web Loader\n",
    "!pip install -qU beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Retrieval part of RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Libraries\n",
    "\n",
    "We need to import three libraries:\n",
    "\n",
    "* `WebBaseLoader`: This tool from the `langchain_community` library allows us to easily fetch content from a website.\n",
    "* `RecursiveCharacterTextSplitter`: This class from `langchain_text_splitters` is used to divide the text into smaller, more manageable chunks. This is crucial for working with LLMs that have limitations on the amount of text they can process at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching and Processing the Information\n",
    "Create a `WebBaseLoader` instance pointed at a specific blog post and then use the `.load()` method to fetch the content from that URL. The loaded content is stored in the `data` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can split the loaded webpage content into smaller chunks using `RecursiveCharacterTextSplitter`.\n",
    "* `chunk_size=500`:  Each chunk will be approximately 500 characters long.\n",
    "* `chunk_overlap=50`: Consecutive chunks will overlap by 50 characters. \n",
    "\n",
    "The overlap helps maintain context between chunks, which can be important for the LLM to understand the text as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the `split_documents()` method of the `text_splitter` object  created earlier. This method does the actual work of dividing the text into chunks according to the `chunk_size` and `chunk_overlap` parameters we specified.\n",
    "\n",
    "Here, we're passing the `data` variable (which holds the loaded webpage content) as input to the `split_documents()` method. This tells the splitter what text it needs to split up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the Data with ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will embed our text chunks and store them in a vector database. This is a crucial step for efficient information retrieval.\n",
    "\n",
    "We'll use `OllamaEmbeddings` to generate these embeddings locally with the `\"nomic-embed-text\"` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "local_embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the `local_embeddings` defined above, we actually need to download this embedding model to your local computer. \n",
    "\n",
    "To do this, run the following command in Ollama terminal:\n",
    "\n",
    ">llama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should create your vectorstore using the `Chroma.from_documents()` method. This is where you'll combine the text splits (`all_splits`) with your chosen embeddings (`local_embeddings`) and store them in Chroma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=local_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we have a working vector store! Test that similarity search is working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.' metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "print(len(docs))\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting with a Local LLM using Ollama\n",
    "\n",
    "Now you should initialize the `ChatOllama` model. This sets up your connection to the Ollama server and specifies which language model you'll be using.\n",
    "\n",
    "We use Ollama with `llama3.1:8b` here, but you can explore other providers or model options depending on your hardware setup.\n",
    "\n",
    "Before running the following cell you should execute the following command in Ollama terminal:\n",
    "\n",
    "> ollama pull llama3.1:8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"mistral:latest\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it to make sure you’ve set everything up properly. In the code below, we use `IPython.display` to render the LLM's response nicely within your Jupyter Notebook environment\n",
    "\n",
    " It's likely that the the responses from llms contain Markdown formatting. The `display()` function, when used with `Markdown(...)`,  tells Jupyter Notebook to interpret and render the text as Markdown. This ensures that any formatting instructions in the LLM's response (like bolding, lists, or headings) are displayed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Omni is a versatile and powerful AI model that can be used in various applications, including audio language modeling. Here's how it can help:\n",
       "\n",
       "1. Speech Recognition: Omni can be trained to transcribe speech from audio files into text, thereby enabling voice-to-text functionality. This is the first step in processing spoken language and can be crucial for developing virtual assistants, call center solutions, or any application where automatic speech recognition is required.\n",
       "\n",
       "2. Speech Synthesis: On the other hand, Omni can also be used to convert text into spoken language, a process known as text-to-speech synthesis. This feature can be useful for applications like audio books, language learning platforms, or even for generating voice prompts in software interfaces.\n",
       "\n",
       "3. Language Understanding: Once the speech is transcribed into text, Omni's natural language processing (NLP) capabilities can help understand the intent and context of the spoken words. This is essential for building intelligent conversational systems like chatbots or virtual assistants that can respond appropriately to user queries.\n",
       "\n",
       "4. Sentiment Analysis: In the context of audio data, Omni can analyze the tone, pitch, and speed of speech to determine the speaker's emotional state, which can help in customer service applications for gauging customer satisfaction.\n",
       "\n",
       "5. Speaker Identification: By analyzing various acoustic features of the voice, such as frequency, intensity, and duration, Omni can help identify different speakers in a given audio file. This is useful for applications like forensics or call center management where multiple people might be speaking at once.\n",
       "\n",
       "In summary, Omni, with its advanced AI capabilities, can greatly facilitate various aspects of audio language modeling, making it easier to process, understand, and generate spoken language data."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "response_message = model.invoke(\n",
    "    \"how omni helps in Audio Language Modeling\"\n",
    ")\n",
    "\n",
    "# Compile the output to markdown\n",
    "markdown_output = response_message.content\n",
    "\n",
    "# Print or display the markdown output\n",
    "display(Markdown(markdown_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Summarization Chain\n",
    "\n",
    "We will create a summarization chain which will produce a summary of the retrieved documents based on the search.\n",
    "\n",
    "First, you'll need to import the necessary modules and define how you want to structure your prompt to the language model.\n",
    "\n",
    "You should create a template for the prompt you'll send to the LLM by using the `from_template` method of the `ChatPromptTemplate`. \n",
    "\n",
    "In this case, the template is *\"Summarize the main themes in these retrieved docs: {docs}*. The `{docs}` part is a placeholder that will be filled with the actual documents retrieved from your vectorstore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"{docs}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need a way to format the documents retrieved from your vectorstore so they can be included in the prompt. This `format_docs` function below takes a list of documents and joins their `page_content` together with double newlines (\\n\\n) as separators. \n",
    "\n",
    "This creates a single string containing the content of all retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert loaded documents into strings by concatenating their content\n",
    "# and ignoring metadata\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's assemble a processing pipeline using LangChain's concise syntax. The statement is missing the most essential pieces. Make an educated guess :)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = {\"docs\": format_docs} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can use your setup to answer a question.\n",
    "\n",
    "First search your vectorstore for documents relevant to the `question` and store them in the `docs` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "docs = vectorstore.similarity_search(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code `invoke`s the `chain` with the retrieved `docs` as **input**. The chain formats the documents, creates the prompt, sends it to the LLM, and parses the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_output = chain.invoke(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the final outcome as Markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " In summary, task decomposition can be achieved in three ways:\n",
       "\n",
       "1. Using simple prompts like \"Steps for XYZ.\\n1.\" or \"What are the subgoals for achieving XYZ?\" to guide the Large Language Model (LLM) to break down complex tasks into smaller and more manageable steps.\n",
       "2. Employing task-specific instructions, such as using \"Write a story outline\" when writing a novel, which helps the model understand the specific requirements of the task at hand.\n",
       "3. Incorporating human inputs, where a human guides or provides information to the LLM throughout the task decomposition process.\n",
       "\n",
       "Chain of thought (CoT) is a popular method for enhancing the performance of models on complex tasks by instructing them to \"think step by step\". CoT helps to decompose hard tasks into smaller and simpler steps, making big tasks more manageable. Additionally, it offers insights into the model's thinking process."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(markdown_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question & Answering \n",
    "\n",
    "Instead of summarization, you can also perform question-answering with your local model and vector store. Here’s an example with a simple string prompt. \n",
    "\n",
    "Pay attention to the content and structure of the prompt. `{context}` and `{question}` are the placeholders to later insert the context information (retrieved from the vector database) and the question asked by the user. Right now they are not known.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_TEMPLATE = \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Context : ```{context}```\n",
    "\n",
    "Answer the following question:\n",
    "\n",
    "{question}\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow two approaches to create the chain. \n",
    "\n",
    "#### First Approach\n",
    "\n",
    "In the first approach, the chain will be invoked via \n",
    "\n",
    "`chain.invoke({\"context\": context, \"question\": question})`. \n",
    "\n",
    "This means, we need to provide the context along with the question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is RunnablePassthrough**\n",
    "\n",
    "Now that we have the prompt template, we need to define a chain of operations to answer questions using relevant context. This chain should \n",
    "\n",
    "1. receive the context and the question from the input dictionary, \n",
    "2. generate an answer using a language model, and \n",
    "3. then extracts the answer as a string.\n",
    "\n",
    "In the chain below, you may notice the `RunnablePassthrough` object. \n",
    "\n",
    "`RunnablePassthrough` is a fundamental component in LangChain that allows you to inject custom logic or data into a chain of language models. It's essentially a placeholder that can be filled with a specific function or value.\n",
    "\n",
    "The RunnablePassthrough is used to preprocess the context before it's fed to the language model.\n",
    "\n",
    "You can think in this way: At the moment the question and the context is not known. Once they are known, they will be passed through the chain. But, without them at the moment, we can still build our pipelien or chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=lambda input: input[\"context\"],\n",
    "        question=lambda input: input[\"question\"],  # Explicitly pass question\n",
    "    )\n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we could just use `RunnablePassthrough()` and the chain would still work. \n",
    "\n",
    "`RunnablePassthrough` receives the input (which is literally a dictionary containing the `question` key, and the `context`) and passes it along unchanged. \n",
    "\n",
    "These two keys then are fed to `rag_prompt` which has the input placeholders (`{question} and {context}`).\n",
    "\n",
    "All these interactions between these components happen automatically in LangChain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "chain = (\n",
    "        RunnablePassthrough() # this still works\n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test our chain. Remember that we will invoke the chain with `chain.invoke({\"context\": context, \"question\": question})`, which means we need to define the question and the context.\n",
    "\n",
    "Question is defind for you, just execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What strategies can be employed to ensure the consistency and alignment between various system specifications, such as requirements, design, and test during development? Provide examples of how these strategies might be applied in a real-world scenario.?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did before, now do a `similarity_search` to find the docs similar to the `question`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docs = vectorstore.similarity_search(question)\n",
    "context = format_docs(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, please execute the following cell to `invoke` the chain to obtain the final response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " To ensure consistency and alignment between various system specifications during development, following strategies can be employed:\n",
       "\n",
       "1. Requirements Traceability Matrix (RTM): This tool helps link the high-level requirements to their corresponding lower-level designs and test cases. By doing this, developers can easily track the progress of each requirement throughout the development life cycle and ensure they are all met.\n",
       "\n",
       "Example: In a real-world scenario, an RTM for a self-driving car system might link the high-level requirement \"The car should safely navigate through traffic\" to its corresponding lower-level designs (e.g., lane detection algorithms) and test cases (e.g., simulated traffic scenarios).\n",
       "\n",
       "2. Design Review: Periodic meetings where designers discuss, review, and critique each other's work can help ensure that the design is coherent and aligned with requirements. This process also allows early identification of potential issues before they become critical during implementation.\n",
       "\n",
       "Example: In a software development project, weekly design reviews might be held to discuss and critique the progress made on different modules or features.\n",
       "\n",
       "3. Test-Driven Development (TDD): Developing tests for new functionalities before actually writing the code can help ensure that the system meets its requirements. This approach also helps in catching issues early and fosters a better design as the focus is on meeting specific behavioral expectations.\n",
       "\n",
       "Example: In a self-driving car project, TDD might involve creating tests to verify the performance of the lane detection algorithms before actually implementing them.\n",
       "\n",
       "By employing these strategies, developers can ensure that their system meets its requirements, has a coherent design, and is thoroughly tested throughout the development life cycle."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "markdown_output = chain.invoke({\"context\": context, \"question\": question})\n",
    "\n",
    "display(Markdown(markdown_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you could format the retrieved docs within the chain. Here is how you could do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1) **Requirements Traceability Matrix (RTM):** This tool helps ensure that each requirement is addressed by a corresponding design or test case, thus maintaining consistency. For instance, if a requirement states \"The system should be able to process images quickly,\" the RTM would link this requirement to its implementation in the design phase (e.g., using a specific algorithm) and testing phase (e.g., performance benchmark tests).\n",
       "\n",
       "2) **Model-Based Development (MBD):** MBD uses formal models of the system, such as UML diagrams or statecharts, to describe requirements, design, and test cases in a unified manner. This approach allows for easy tracking of changes and ensures consistency across different stages of development. For example, if a change is made to the design during the implementation phase, the corresponding models can be updated automatically, ensuring that the new design adheres to the original requirements.\n",
       "\n",
       "3) **Test-Driven Development (TDD):** TDD emphasizes writing tests before the actual code, thereby ensuring that the system meets its specified requirements. As the development progresses, the tests are continuously updated and rerun to verify that no unintended changes have affected the system's behavior. For instance, if a new feature is introduced in the design phase, corresponding test cases would be written first before any implementation work begins."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chain = (\n",
    "    RunnablePassthrough.assign(context=lambda input: format_docs(input[\"context\"]))\n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"What strategies can be employed to ensure the consistency and alignment between various system specifications, such as requirements, design, and test during development? Provide examples of how these strategies might be applied in a real-world scenario.?\"\n",
    "\n",
    "docs = vectorstore.similarity_search(question)\n",
    "\n",
    "\n",
    "markdown_output = chain.invoke({\"context\": docs, \"question\": question})\n",
    "\n",
    "display(Markdown(markdown_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, pay attention that, we did not explicitly pass the `question`. We did pass `context` since we had to apply `format_docs` on the retrieved documents before feeding them to `rag_prompt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Approach\n",
    "\n",
    "As the second approach, instead of manually passing in docs, you can automatically retrieve them from our vector store based on the user question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "qa_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How the Chain Executes:**\n",
    "\n",
    "* When you invoke the `qa_chain`, you likely pass a dictionary like this: `qa_chain.invoke({\"question\": \"What is the capital of France?\"})`.\n",
    "* The `{\"context\": ..., \"question\": ...}` step receives this entire dictionary as its input.\n",
    "* `RunnablePassthrough()` assigns the whole input dictionary `{\"question\": \"What is the capital of France?\"}` to the `\"question\"` key.\n",
    "* The `retriever` also receives the same whole dictionary as input. Since the `retriever` is designed to extract and use the `\"question\"` key from the input dictionary, it uses *\"What is the capital of France?\"* for its similarity search.\n",
    "* The retrieved documents (from retriever) are then formatted by `format_docs` and assigned to the `\"context\"` key.\n",
    "* Finally, the resulting dictionary `{\"context\": formatted_docs, \"question\": {\"question\": \"What is the capital of France?\"}}` is passed to `rag_prompt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. Chain of Thought (CoT) prompting, where the model is instructed to break down complex tasks into smaller steps.\n",
       "2. Using task-specific instructions that guide the model towards a particular goal, such as writing a story outline for a novel.\n",
       "3. Human inputs can also be used in task decomposition to provide additional guidance or context to the model."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "\n",
    "display(Markdown(qa_chain.invoke(question)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Llama.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practical applications of LLMs can be limited by the need for high-powered computing or the necessity for quick response times. These models typically require sophisticated hardware and extensive dependencies, which can make difficult their adoption in more constrained environments.\n",
    "\n",
    "This is where LLaMa.cpp (or LLaMa C++) comes to the rescue, providing a lighter, more portable alternative to the heavyweight frameworks.\n",
    "\n",
    "Llama.cpp was developed by Georgi Gerganov. It implements the Meta’s LLaMa architecture in efficient C/C++, and it is one of the most dynamic open-source communities around the LLM inference with more than 900 contributors, 69000+ stars on the official GitHub repository, and 2600+ releases.\n",
    "\n",
    "Llama.cpp’s backbone is the original Llama models, which is also based on the transformer architecture. The authors of Llama leverage various improvements that were subsequently proposed and used different models such as PaLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first install the Python binding for llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.3.5.tar.gz (64.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.5/64.5 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /opt/anaconda3/envs/math_assistant/lib/python3.11/site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/anaconda3/envs/math_assistant/lib/python3.11/site-packages (from llama-cpp-python) (1.26.4)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /opt/anaconda3/envs/math_assistant/lib/python3.11/site-packages (from llama-cpp-python) (3.1.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/math_assistant/lib/python3.11/site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.5-cp311-cp311-macosx_15_0_arm64.whl size=3049304 sha256=3ee067e8004f8f785a682fdaba8e77c4f986f6ac5c199e722720e4d0e1405ce0\n",
      "  Stored in directory: /Users/dogukanince/Library/Caches/pip/wheels/12/8a/8d/ea0c2c1a2a663d41431988f6e92de7d7f479d4e95a5c49fe85\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.5\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Llama class imported above is the main constructor leveraged when using Llama.cpp, and it takes several parameters and is not limited to the ones below. The complete list of parameters is provided in the official documentation:\n",
    "\n",
    "* `model_path`: The path to the Llama model file being used\n",
    "* `prompt`: The input prompt to the model. This text is tokenized and passed to the model.\n",
    "* `device`: The device to use for running the Llama model; such a device can be either CPU or GPU.\n",
    "* `max_tokens`: The maximum number of tokens to be generated in the model’s response\n",
    "* `stop`: A list of strings that will cause the model generation process to stop\n",
    "* `temperature`: This value ranges between 0 and 1. The lower the value, the more deterministic the end result. On the other hand, a higher value leads to more randomness, hence more diverse and creative output.\n",
    "* `top_p`: Is used to control the diversity of the predictions, meaning that it selects the most probable tokens whose cumulative probability exceeds a given threshold. Starting from zero, a higher value increases the chance of finding a better output but requires additional computations.\n",
    "* `echo`: A boolean used to determine whether the model includes the original prompt at the beginning (True) or does not include it (False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you use Ollama to download LLMs, they are stored in a specific directory on your system (in Windows): `C:/Users/<<windows user>>/.ollama/models/`. \n",
    "\n",
    "**Important:**  You'll need to replace `<<your_username>>` with your actual Windows username.\n",
    "\n",
    "For Mac users, The default storage location for these models is within the user's home directory, specifically under `~/.ollama/models`.\n",
    "\n",
    "This directory, known as the `model_path`, is crucial for accessing your LLMs. To ensure your code functions correctly, you must update the `model_path` variable in your scripts to point to this location. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LlamaCpp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Make sure the model path is correct for your system!\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m llm_llamacpp \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaCpp\u001b[49m(\n\u001b[1;32m      3\u001b[0m     model_path\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPATH_TO_YOUR_MODEL\u001b[39m\u001b[38;5;124m\"\u001b[39m,    \n\u001b[1;32m      4\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      5\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2500\u001b[39m,\n\u001b[1;32m      6\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      7\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Verbose is required to pass to the callback manager\u001b[39;00m\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LlamaCpp' is not defined"
     ]
    }
   ],
   "source": [
    "# Make sure the model path is correct for your system!\n",
    "llm_llamacpp = LlamaCpp(\n",
    "    model_path= \"PATH_TO_YOUR_MODEL\",    \n",
    "    temperature=1,\n",
    "    max_tokens=2500,\n",
    "    top_p=1,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the same chain as before but replace the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain_llamacpp = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm_llamacpp\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the code to invoke the model with a question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   23052.11 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   369 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    30 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   25731.14 ms /   399 tokens\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       " \n",
       "\n",
       "Assistant: Task decomposition can be approached in three ways: using simple prompting for LLMs, applying task-specific instructions, or incorporating human inputs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "\n",
    "display(Markdown(qa_chain_llamacpp.invoke(question)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed Gains with LLaMa.cpp\n",
    "\n",
    "Have you noticed the improved speed when using LLaMa.cpp? Its optimized C++ implementation often leads to faster responses compared to Ollama.\n",
    "\n",
    "LLaMa.cpp, with its efficient C++ implementation, is designed for speed and often outperforms Ollama in benchmarks. This can be particularly important for applications that require real-time interactions or rapid responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
