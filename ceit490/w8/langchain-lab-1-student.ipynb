{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb16c5e6-c629-4d1d-9ce1-62050a20fec0",
   "metadata": {},
   "source": [
    "# M7 - LangChain Lab Assignment\n",
    "\n",
    "This is a simple assignment to help you practice few-shot prompting with LangChain.\n",
    "\n",
    "You need to create a few-shot prompt templates and test them with models from Hugging Face models and GROQ to produce antoynms of given English words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5ff36e-4acb-44f1-87f1-8645afeb302c",
   "metadata": {},
   "source": [
    "## 1. Preparation\n",
    "\n",
    "1. Set the `GROQ_API_KEY` value using the menu options in DataLab: `Environment -> Environment variables`. You can obtain an api key from https://console.groq.com/keys.\n",
    "\n",
    "2. Set the `HF_API` value using the same approach. You can obtain API key from Hugging Face portal.\n",
    "\n",
    "4. Run the `pip install -U langchain` command in the following cell, and after the installation is completed, go to the `Run -> Restart Kernel` in the DataLab menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e035d20-02ca-4d16-b0b9-53b3b8a35c62",
   "metadata": {
    "collapsed": true,
    "executionCancelledAt": null,
    "executionTime": 6838,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1733060628029,
    "lastExecutedByKernel": "f051fe43-db1c-4b6c-befa-a903d0ebaf54",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "pip install -U langchain",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (0.3.9)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from langchain) (3.10.8)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from langchain) (0.3.21)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from langchain) (0.1.130)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.13.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.21->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.21->langchain) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.21->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: anyio in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain) (2.4)\n",
      "Requirement already satisfied: exceptiongroup in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "624cc053-27c4-4eca-b7f8-f24bb5a6f517",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 1565,
    "lastExecutedAt": 1733060635574,
    "lastExecutedByKernel": "f051fe43-db1c-4b6c-befa-a903d0ebaf54",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import langchain"
   },
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc55603-c08b-4746-acf5-457f12f16e51",
   "metadata": {},
   "source": [
    "## 2. Using MessagesPlaceholder with a Model from GROQ\n",
    "\n",
    "Let's first implement few-shot prompting with `MessagesPlaceholder` and try it with a model from Groq Cloud.\n",
    "\n",
    "In this part, first **import** the following items properly: `ChatPromptTemplate` and `MessagesPlaceholder` from (`langchain_core.prompts`); `HumanMessage`, `SystemMessage` and `AIMessage` from `langchain_core.messages`. Please check the LangChain documentation if you need help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2492d4d-cd8a-41ed-bd9a-f0da90f22f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements goes here\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate, FewShotPromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc514f2-05b3-4cca-983f-c6bf630cde74",
   "metadata": {},
   "source": [
    "Then, in the following cell, create a `ChatPromptTemplate` object, assigned to `prompt_template`. \n",
    "\n",
    "It should contain a `SystemMessage` with a proper `content` (_remember that this assistant is good at providing antonyms._). \n",
    "\n",
    "Also, the `ChatPromptTemplate` object should have a `MessagePlaceholder` named _\"messages\"_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60fb6ed8-1108-4300-b4ab-8a32b3d1c827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here.\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a helpful assistant and good at providing antonyms.\"),\n",
    "    HumanMessage(content=\"Tell me the antonym of {word}.\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045ba47d-4c02-44bd-950d-df3fa0154a34",
   "metadata": {},
   "source": [
    "Create a list named `messages` to store a conversation between a human and an AI. In this conversation, the human provides a word, and the AI responds with its antonym. This represents few-shot prompting.\n",
    "\n",
    "Each message should be represented using either `HumanMessage` or `AIMessage`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2111c64-8f04-4dd1-b176-e1019e8f84fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    HumanMessage(content=\"Tell me the antonym of happy.\"),\n",
    "    AIMessage(content=\"sad\"),\n",
    "    HumanMessage(content=\"Tell me the antonym of fast.\"),\n",
    "    AIMessage(content=\"slow\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def81101-65d8-434d-8d4b-58919e573e81",
   "metadata": {},
   "source": [
    "Now that the prompt is ready, we can test it with an LLM. To do this, we will use GROQ. Please run the following code to install `langchain_groq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3bf1234-8091-41b6-80de-ac06f1880b33",
   "metadata": {
    "collapsed": true,
    "executionCancelledAt": null,
    "executionTime": 2921,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1733060646643,
    "lastExecutedByKernel": "f051fe43-db1c-4b6c-befa-a903d0ebaf54",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "pip install langchain_groq",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_groq in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (0.2.1)\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from langchain_groq) (0.13.0)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from langchain_groq) (0.3.21)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain_groq) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain_groq) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain_groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain_groq) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain_groq) (4.12.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_groq) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_groq) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_groq) (0.1.130)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_groq) (23.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_groq) (8.5.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (3.6)\n",
      "Requirement already satisfied: exceptiongroup in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (1.2.0)\n",
      "Requirement already satisfied: certifi in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_groq) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_groq) (3.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_groq) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_groq) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_groq) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dogukanince/miniforge3/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_groq) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_groq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500130b5-5f6b-4588-97ed-b96227a02c14",
   "metadata": {},
   "source": [
    "Next, create a `ChatGroq` object assigned to `groq_llm`. The `model` should be `llama-3.1-70b-versatile` and the `temperature` should be `0.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6470d9c-e644-44bc-b1cd-6dcad5a34961",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Completions.create() got an unexpected keyword argument 'chat_prompt_template'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m      4\u001b[0m groq_api_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgsk_jTDZre9XfP5XSbAOwqahWGdyb3FYs5RVb2OHUn9Cf5picdAGlVNi\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatGroq(\n\u001b[1;32m      7\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m      8\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-3.1-70b-versatile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     groq_api_key\u001b[38;5;241m=\u001b[39mgroq_api_key,\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchat_prompt_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat_prompt_template\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    285\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/langchain_groq/chat_models.py:474\u001b[0m, in \u001b[0;36mChatGroq._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    470\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    472\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    473\u001b[0m }\n\u001b[0;32m--> 474\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "\u001b[0;31mTypeError\u001b[0m: Completions.create() got an unexpected keyword argument 'chat_prompt_template'"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "\n",
    "groq_api_key = \"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\"\n",
    "\n",
    "llm = ChatGroq(\n",
    "    temperature=0.0,\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    groq_api_key=groq_api_key,\n",
    ")\n",
    "\n",
    "llm.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0a5e5e-bc07-4806-9f39-ce2067df3b37",
   "metadata": {},
   "source": [
    "Combine the `prompt_template` and the `groq_llm` to create a chain. Assign this chain to a variable called `chain`. Use the `|` operator to create the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "107a0e36-9733-4562-af63-b99acecee46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = llm | chat_prompt_template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2199914d-0bc7-41c8-a4c6-d79d7d6c8380",
   "metadata": {},
   "source": [
    "Now, it is time to get the user for an English word. This is done in the following cell using the `input` function. The word entered by the user is then stored in a variable called `new_word`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37cb4739-fbf0-49a7-85d1-ff89e8c17c7b",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 5321,
    "lastExecutedAt": 1733060664017,
    "lastExecutedByKernel": "f051fe43-db1c-4b6c-befa-a903d0ebaf54",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "new_word = input(\"Enter an english word:\")",
    "outputsMetadata": {
     "0": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "new_word = \"Hardworking\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aedb6d-8689-435b-8a8a-940eb6e72f96",
   "metadata": {},
   "source": [
    "Create a new `HumanMessage` using the `new_word` entered by the user.\n",
    "\n",
    "Append this new `HumanMessage` to your existing `messages` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be0b2c1a-ec11-4ae6-8ab8-d4b765a1fd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(HumanMessage(content=new_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f31e6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Tell me the antonym of happy.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='sad', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Tell me the antonym of fast.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='slow', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hardworking', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5feb46e-751d-4892-8b40-a2023da3b377",
   "metadata": {},
   "source": [
    "Use the `chain.invoke()` method with the updated messages list to get the AI's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad5920e5-eab8-4a2e-9e95-9724ded67080",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Invoke the chain with the new word\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_word\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/langchain_core/runnables/base.py:3022\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3020\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m   3021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 3022\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3023\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3024\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:287\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    285\u001b[0m         ChatGeneration,\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m--> 287\u001b[0m             [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m],\n\u001b[1;32m    288\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    289\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    290\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    291\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    292\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    293\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    294\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    295\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:273\u001b[0m, in \u001b[0;36mBaseChatModel._convert_input\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust be a PromptValue, str, or list of BaseMessages.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m     )\n\u001b[0;32m--> 273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages."
     ]
    }
   ],
   "source": [
    "# Invoke the chain with the new word\n",
    "response = chain.invoke({\"word\": new_word})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1149be7b-b563-4196-90d6-e3d89db1a3dd",
   "metadata": {},
   "source": [
    "After we have the response, we will use the `StrOutputParser` class to handle the output of language models and extract the string content. The code is already writen below. Explanations are provided with in-line comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "952acb1b-79ff-4adf-934f-5d5a288093b3",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 12,
    "lastExecutedAt": 1733064069972,
    "lastExecutedByKernel": "f051fe43-db1c-4b6c-befa-a903d0ebaf54",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from langchain_core.output_parsers.string import StrOutputParser\n\noutput_parser = StrOutputParser()\n\n#takes the response object (which is assumed to be the output from a language model) and extracts the text content from it. The extracted text is then stored in the text_output variable.\ntext_output = output_parser.parse(response)\n\n#actual response is stored inside the content attribute of the text_output object\nprint(text_output.content)",
    "outputsMetadata": {
     "0": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m output_parser \u001b[38;5;241m=\u001b[39m StrOutputParser()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#takes the response object (which is assumed to be the output from a language model) and extracts the text content from it. The extracted text is then stored in the text_output variable.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m text_output \u001b[38;5;241m=\u001b[39m output_parser\u001b[38;5;241m.\u001b[39mparse(\u001b[43mresponse\u001b[49m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#actual response is stored inside the content attribute of the text_output object\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(text_output\u001b[38;5;241m.\u001b[39mcontent)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "#takes the response object (which is assumed to be the output from a language model) and extracts the text content from it. The extracted text is then stored in the text_output variable.\n",
    "text_output = output_parser.parse(response)\n",
    "\n",
    "#actual response is stored inside the content attribute of the text_output object\n",
    "print(text_output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917c73f3-f038-4ef6-bcac-9c205fdd9972",
   "metadata": {},
   "source": [
    "## 2. Using FewShotPromptTemplate with a Model from Hugging Face\n",
    "\n",
    "In the second part, let's use the `FewShotPromptTemplate` class from LangChain and test it with an LLM from Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81360e35-2851-447b-9aef-773c6c85f2eb",
   "metadata": {},
   "source": [
    "First **import** the following items properly: `PromptTemplate` and `FewShotPromptTemplate` from `langchain_core.prompts`. Please check the LangChain documentation if you need help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ec96a0e-f912-4782-910b-7cf4d653fb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statement goes here\n",
    "\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate, FewShotPromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dd86e1-81cf-42d0-a7cf-b7500cb71517",
   "metadata": {},
   "source": [
    "Before we can use `FewShotPromptTemplate`, we should define the `examples` list, in which each example is provided as a dictionary with `\"word\"` and `\"antonym\"` keys. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7a8d778-6f78-4d0c-9af0-6cdf6b648b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"word\": \"long\", \"antonym\": \"short\"}, \n",
    "    {\"word\": \"wide\", \"antonym\": \"narrow\"},\n",
    "    {\"word\": \"fat\", \"antonym\": \"skinny\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0360b7-a25c-435d-b846-a39a0fa614ea",
   "metadata": {},
   "source": [
    "Next, we should create the an `example_prompt` using `PromptTemplate`, in which `input_variables` is `\"word\"`. That is, later, to use this prompt, we have to `invoke` it with a `word`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22191b11-5a2f-4158-b00d-073e5acc8ce2",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 54,
    "lastExecutedAt": 1733063764776,
    "lastExecutedByKernel": "f051fe43-db1c-4b6c-befa-a903d0ebaf54",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# create a prompt example from above template\nexample_prompt = PromptTemplate(\n    input_variables=[\"word\"],\n    template= \"User: {word}\\n AI: {antonym}\"\n)"
   },
   "outputs": [],
   "source": [
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"word\"],\n",
    "    template= \"User: {word}\\n AI: {antonym}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bac0f87-4a73-4243-9dc0-5103a2784bb8",
   "metadata": {},
   "source": [
    "Next we should define the prefix and suffix. These are already defined below. The `prefix` text goes before the examples, and the `suffix` text goes after the examples in the final prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae0f5e20-ed12-4d32-a430-9787d8537ea0",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 47,
    "lastExecutedAt": 1733066813107,
    "lastExecutedByKernel": "f051fe43-db1c-4b6c-befa-a903d0ebaf54",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# the prefix is our instructions\nprefix = \"\"\"You are an helpful  assistant good at providing antonyms of a given English word. You must only provide the antonym for a given word. Do not suggest other word and antonym pairs. The following are examples to show you how you should respond: \n\"\"\"\n# and the suffix our user input and output indicator\nsuffix = \"\"\"\nUser: {word}\nAI: \"\"\""
   },
   "outputs": [],
   "source": [
    "# the prefix is our instructions\n",
    "prefix = \"\"\"You are an helpful  assistant good at providing antonyms of a given English word. You must only provide the antonym for a given word. Do not suggest other word and antonym pairs. The following are examples to show you how you should respond: \n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {word}\n",
    "AI: \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb331978-d42f-4902-b24c-de873dfde678",
   "metadata": {},
   "source": [
    "Now, we are ready to create the final template using `FewShotPromptTemplate` . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9ec9927-56b0-4e8b-8d9f-fc2993a6c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix= prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"word\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad3ebc4-61f2-45d2-93ea-2c2b98480611",
   "metadata": {},
   "source": [
    "In the code above, the `FewShotPromptTemplate` object takes in the few-shot examples (`examples`) and the formatter (`example_prompt`) for the few-shot examples. When this `FewShotPromptTemplate` is formatted, it formats the passed examples using the `example_prompt`, then and adds them to the final prompt before suffix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6906d796-73c0-4f49-9e71-7dd677e02311",
   "metadata": {},
   "source": [
    "Now `invoke` the `few_shot_prompt_template` properly. Remember that you need to pass a value for the required input using a dictionary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "392ba823-5c5d-475e-afaf-66e5433f1dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='You are an helpful  assistant good at providing antonyms of a given English word. You must only provide the antonym for a given word. Do not suggest other word and antonym pairs. The following are examples to show you how you should respond: \\n\\n\\nUser: long\\n AI: short\\n\\nUser: wide\\n AI: narrow\\n\\nUser: fat\\n AI: skinny\\n\\n\\nUser: tall\\nAI: ')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_prompt_template.invoke({\"word\": \"tall\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97009b08-9dad-48b1-8bb5-50e8e3bfdc28",
   "metadata": {},
   "source": [
    "So far, we have preapred the `few_shot_prompt_template` and we made sure that it runs OK in the previous code cell.\n",
    "\n",
    "Now, it is time to use it with an LLM (from Hugging Face). To do this, we will first read the `HF_TOKEN` stored as environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f50c0a87-0b01-4ba5-869e-2371a9348dee",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 12,
    "lastExecutedAt": 1733061167269,
    "lastExecutedByKernel": "f051fe43-db1c-4b6c-befa-a903d0ebaf54",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import os\n\nHF_API = os.environ['HF_TOKEN']"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HF_API = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286f1a7a-c990-49f1-97d7-e3486642e876",
   "metadata": {},
   "source": [
    "Next, you should install `langchain_huggingface` using the following statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d43545d5-a430-4952-b73e-5259775e7627",
   "metadata": {
    "collapsed": true,
    "executionCancelledAt": null,
    "executionTime": 2784,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1733060779737,
    "lastExecutedByKernel": "f051fe43-db1c-4b6c-befa-a903d0ebaf54",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "pip install langchain_huggingface",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_huggingface\n",
      "  Downloading langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting huggingface-hub>=0.23.0 (from langchain_huggingface)\n",
      "  Downloading huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from langchain_huggingface) (0.3.21)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from langchain_huggingface) (2.6.1)\n",
      "Collecting tokenizers>=0.19.1 (from langchain_huggingface)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: transformers>=4.39.0 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from langchain_huggingface) (4.39.3)\n",
      "Requirement already satisfied: filelock in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (6.0.1)\n",
      "Requirement already satisfied: requests in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.1.147)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (2.10.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (8.2.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.4.0)\n",
      "Requirement already satisfied: numpy in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.0.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n",
      "Requirement already satisfied: scipy in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.13.0)\n",
      "Requirement already satisfied: Pillow in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (10.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from transformers>=4.39.0->langchain_huggingface) (2023.12.25)\n",
      "INFO: pip is looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting transformers>=4.39.0 (from langchain_huggingface)\n",
      "  Using cached transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting tokenizers>=0.19.1 (from langchain_huggingface)\n",
      "  Downloading tokenizers-0.20.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from transformers>=4.39.0->langchain_huggingface) (0.4.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (2.4)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.10.1)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2024.2.2)\n",
      "Requirement already satisfied: sympy in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (68.2.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.4.0)\n",
      "Requirement already satisfied: anyio in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (4.3.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n",
      "Downloading langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
      "Downloading huggingface_hub-0.26.3-py3-none-any.whl (447 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m447.6/447.6 kB\u001b[0m \u001b[31m744.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "Downloading tokenizers-0.20.3-cp312-cp312-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m550.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers, langchain_huggingface\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.22.2\n",
      "    Uninstalling huggingface-hub-0.22.2:\n",
      "      Successfully uninstalled huggingface-hub-0.22.2\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.2\n",
      "    Uninstalling tokenizers-0.15.2:\n",
      "      Successfully uninstalled tokenizers-0.15.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.39.3\n",
      "    Uninstalling transformers-4.39.3:\n",
      "      Successfully uninstalled transformers-4.39.3\n",
      "Successfully installed huggingface-hub-0.26.3 langchain_huggingface-0.1.2 tokenizers-0.20.3 transformers-4.46.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddd4357-34c6-4400-8624-d4e861e808c9",
   "metadata": {},
   "source": [
    "Then, please import the `HuggingFaceEndpoint` class from the `langchain_huggingface` library. Following this,  create an instance of the `HuggingFaceEndpoint` class and assigns it to the variable `hf_llm`. _This object will be used to communicate with the model._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9ed7329-be79-4816-a75d-c92cf2799082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "hf_llm = HuggingFaceEndpoint(\n",
    "    repo_id='microsoft/Phi-3.5-mini-instruct',\n",
    "    huggingfacehub_api_token = HF_API,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe40062-22cf-4db1-ac71-8dfa094b4d4f",
   "metadata": {},
   "source": [
    "Next, you should create a simple LangChain chain by combining your `few_shot_prompt_template` with the `hf_llm` (Hugging Face language model) you defined earlier. Remember that you should use the pipe operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b53fc14a-38c6-4ece-b8e0-23b00bb62c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = few_shot_prompt_template | hf_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b640d0d1-3361-4f39-af6d-09ac2a20762e",
   "metadata": {},
   "source": [
    "Now, it is time to test our model with our few-shot prompt. Write the necessary code to send a request to a language model (Phi-3.5-mini-instruct) to generate the antonym of the word \"thick\" and store the model's response in the `response` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e89fdac3-1105-4c84-8ac5-5fdcfd084c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "new_word = \"short\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2240d4-cc1f-4477-9842-9200f55db064",
   "metadata": {},
   "source": [
    "As the final task, please print the `response`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d4a0e0-d667-49c8-9d2e-a08942dc2da2",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 54,
    "lastExecutedAt": 1733066853743,
    "lastExecutedByKernel": "f051fe43-db1c-4b6c-befa-a903d0ebaf54",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Your code goes here",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "(Request ID: d9T0DfC5pNHANvzYh6b4I)\n\nBad request:\nAuthorization header is correct, but the token seems invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.12/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://api-inference.huggingface.co/models/microsoft/Phi-3.5-mini-instruct",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Your code goes here\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtall\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m      3\u001b[0m response\n",
      "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.12/site-packages/langchain_core/runnables/base.py:3024\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3023\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3024\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3026\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.12/site-packages/langchain_core/language_models/llms.py:390\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    387\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    388\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 390\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    391\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    392\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    393\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    394\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    395\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    396\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    397\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    398\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    399\u001b[0m         )\n\u001b[1;32m    400\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    402\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.12/site-packages/langchain_core/language_models/llms.py:755\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    749\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    753\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    754\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.12/site-packages/langchain_core/language_models/llms.py:950\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    936\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    937\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    938\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    948\u001b[0m         )\n\u001b[1;32m    949\u001b[0m     ]\n\u001b[0;32m--> 950\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[1;32m    951\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    952\u001b[0m     )\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.12/site-packages/langchain_core/language_models/llms.py:792\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    791\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 792\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    793\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.12/site-packages/langchain_core/language_models/llms.py:779\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    770\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    771\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    776\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    778\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 779\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m    780\u001b[0m                 prompts,\n\u001b[1;32m    781\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    782\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[1;32m    783\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    784\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    785\u001b[0m             )\n\u001b[1;32m    786\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    787\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    788\u001b[0m         )\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.12/site-packages/langchain_core/language_models/llms.py:1502\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m   1501\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1502\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1503\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1504\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1505\u001b[0m     )\n\u001b[1;32m   1506\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.12/site-packages/langchain_huggingface/llms/huggingface_endpoint.py:312\u001b[0m, in \u001b[0;36mHuggingFaceEndpoint._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     invocation_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m invocation_params[\n\u001b[1;32m    310\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    311\u001b[0m     ]  \u001b[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m    313\u001b[0m         json\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m: invocation_params},\n\u001b[1;32m    314\u001b[0m         stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    315\u001b[0m         task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask,\n\u001b[1;32m    316\u001b[0m     )\n\u001b[1;32m    317\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:296\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 296\u001b[0m     hf_raise_for_status(response)\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:460\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[1;32m    457\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m endpoint:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m endpoint_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m     )\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(BadRequestError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m    463\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure your token has the correct permissions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    467\u001b[0m     )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: (Request ID: d9T0DfC5pNHANvzYh6b4I)\n\nBad request:\nAuthorization header is correct, but the token seems invalid"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "response = chain.invoke({\"word\": \"tall\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007b789c-d658-4b5d-9b1e-405ce28e42db",
   "metadata": {},
   "source": [
    "Probably, you could not get the desired response with `Phi-3.5-mini-instruct`, a very small LLM compared to its competitors such as GPT-4 or Llama 3.2.\n",
    "\n",
    "I think in most cases, small LLMs can be used for very simple tasks. Finding an antonym could be even a quite complex task for them.\n",
    "\n",
    "However, they may function better if they are fine-tuned. Fine-tuning on specific tasks can greatly enhance their performance, potentially surpassing larger models in those narrow domains.\n",
    "\n",
    "Their biggest advantage is that they require less computational power, making them ideal for deployment on less powerful devices or for applications with resource constraints."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "editor": "DataLab",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
