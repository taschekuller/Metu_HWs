{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d61d60c9-b353-4385-b097-aefb81ad32e0",
   "metadata": {},
   "source": [
    "# Building a simple Agent with Tools and Toolkits in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a6a9ff-66c5-439d-83a4-5570311dca47",
   "metadata": {},
   "source": [
    "## Configuring the environment and installing  \n",
    "\n",
    "First create a new Environment using the menu. Add the following information in the new environment:  \n",
    "\n",
    "``` \n",
    "Environment Variables Set Name : OpenAI  \n",
    "NAME    : OPENAI_API_KEY \n",
    "VALUE  : ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3d1e3f-4237-46eb-b37e-1e9b99ef3deb",
   "metadata": {},
   "source": [
    "## What are Agents\n",
    "\n",
    "If you lookup the definition of AI Agents, you get something along the lines of “An entity that is able to perceive its environment, act on its environment, and make intelligent decisions about how to reach a goal it has been given, as well as the ability to learn as it goes”\n",
    "\n",
    "That fits the definition of LangChain agents pretty well. What makes all this possible in software is the reasoning abilities of Large Language Model’s (LLM’s). The brains of a LangChain agent are an LLM. It is the LLM that is used to reason about the best way to carry out the ask requested by a user.\n",
    "\n",
    "In order to carry out its task, and operate on things and retrieve information, the agent has `Tool`s. It is through these tools that it is able to interact with its environment.\n",
    "\n",
    "The tools are basically just methods/classes the agent has access to that can do things like interact with a Stock Market index over an API, update a Google Calendar event, or run a query against a database. We can build out tools as needed, depending on the nature of tasks we are trying to carry out with the agent to fulfil.\n",
    "\n",
    "A collection of Tools in LangChain are called a Toolkit. Implementation wise, this is literally just an array of the Tools that are available for the agent.\n",
    "\n",
    "So, at a basic level, an agent needs\n",
    "\n",
    "* an LLM to act as its brain, and to give it its reasoning abilities\n",
    "* tools so that it can interact with the environment around it and achieve its goals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252e6bd5-b30e-4c14-918f-da850434d368",
   "metadata": {},
   "source": [
    "## Building the Agent\n",
    "To make some of these concepts more concrete, let’s build a simple agent.\n",
    "\n",
    "We will create a Mathematics Agent that can perform a few simple mathematical operations.\n",
    "\n",
    "First let's install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6d4e8fb-47fd-49bd-a912-ce9e58f13eb0",
   "metadata": {
    "collapsed": true,
    "executionCancelledAt": null,
    "executionTime": 8751,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1735098904725,
    "lastExecutedByKernel": "c816e52d-560a-49bb-84da-f898f6c17d63",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "pip install langchain langchain_openai",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (0.3.9)\n",
      "Collecting langchain_openai\n",
      "  Downloading langchain_openai-0.2.14-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from langchain) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from langchain) (3.9.4)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from langchain) (0.3.21)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from langchain) (0.3.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from langchain) (0.1.147)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from langchain) (2.0.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from langchain) (2.10.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from langchain) (8.2.3)\n",
      "Collecting langchain-core<0.4.0,>=0.3.21 (from langchain)\n",
      "  Downloading langchain_core-0.3.28-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting openai<2.0.0,>=1.58.1 (from langchain_openai)\n",
      "  Using cached openai-1.58.1-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
      "  Using cached tiktoken-0.8.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.21->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.21->langchain) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.21->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.1)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.9.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.58.1->langchain_openai)\n",
      "  Using cached jiter-0.8.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (4.66.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2023.12.25)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain) (2.4)\n",
      "Downloading langchain_openai-0.2.14-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m581.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.28-py3-none-any.whl (411 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached openai-1.58.1-py3-none-any.whl (454 kB)\n",
      "Using cached tiktoken-0.8.0-cp312-cp312-macosx_11_0_arm64.whl (982 kB)\n",
      "Using cached jiter-0.8.2-cp312-cp312-macosx_11_0_arm64.whl (310 kB)\n",
      "Installing collected packages: jiter, tiktoken, openai, langchain-core, langchain_openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.19.0\n",
      "    Uninstalling openai-1.19.0:\n",
      "      Successfully uninstalled openai-1.19.0\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.21\n",
      "    Uninstalling langchain-core-0.3.21:\n",
      "      Successfully uninstalled langchain-core-0.3.21\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-community 0.0.33 requires langchain-core<0.2.0,>=0.1.43, but you have langchain-core 0.3.28 which is incompatible.\n",
      "langchain-community 0.0.33 requires numpy<2,>=1, but you have numpy 2.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed jiter-0.8.2 langchain-core-0.3.28 langchain_openai-0.2.14 openai-1.58.1 tiktoken-0.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c739017e-ce08-4c95-9408-dfeac3322b65",
   "metadata": {},
   "source": [
    "> Please restart the kernal after the installations: `Run -> Restart Kernal`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba177c5-070a-48c1-b38f-7a0a6d930923",
   "metadata": {},
   "source": [
    "Just execute the cell below to import the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc24c452",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8610b39-9837-4049-a1a4-8bab75dbb2b2",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastExecutedByKernel": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages/langchain/chains/api/base.py:56: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_community.utilities.requests import TextRequestsWrapper\n",
      "/Users/dogukanince/miniforge3/envs/test_env/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:777: UserWarning: Mixing V1 models and V2 models (or constructs, like `TypeAdapter`) is not supported. Please upgrade `TextRequestsWrapper` to V2.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e5e249-5c93-4753-80e2-eb929ac77eb0",
   "metadata": {},
   "source": [
    "## The Tools\n",
    "The simplest place to start will be to fist define the tools for our Maths agent.\n",
    "\n",
    "Let’s give it “add”, “multiply” and “square” tools, so that it can perform those operations on questions we pass to it. By keeping our tools simple we can focus on the core concepts, and build the tools ourselves, instead of relying on an existing and more complex tools like the WikipediaTool, that acts as a wrapper around the Wikipedia API, and requires us to import it from the LangChain library.\n",
    "\n",
    "Again, we are not trying to do anything fancy here, just keeping it simple and putting the main building blocks of an agent together so we can understand how they work, and get our first agent up and running.\n",
    "\n",
    "Let’s start with the “add” tool. LangChain does give us an easier way to define tools, then by needing to extend the BaseTool class each time. We can do this with the help of the @tool decorator. Defining the “add” tool in LangChain using the @tool decorator will look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04d3e594-34f6-4907-bc20-198288eda38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22c1a5c-1903-4cd6-afcd-cfe8537085cf",
   "metadata": {},
   "source": [
    "Some thing to note:\n",
    "\n",
    "* the method name also becomes the tool name\n",
    "* the method params define the input parameters for the tool\n",
    "* the docstring gets converted into the tools description\n",
    "\n",
    "You can access these properties (`name`, `description`, and `args`) on the tool also:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e489ac2a-25f0-42eb-8193-7ba8f29e16b0",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastExecutedByKernel": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 80,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add\n",
      "Add two numbers.\n",
      "{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n"
     ]
    }
   ],
   "source": [
    "print(add.name) \n",
    "print(add.description)\n",
    "print(add.args) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57040a7-bc67-4a4c-afc4-cd279984999b",
   "metadata": {},
   "source": [
    "Note that the description of a tool is very important as this is what the LLM uses to decide whether or not it is the right tool for the job. A bad description may lead to the not tool getting used when it should be, or getting used at the wrong times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c139a85e-832e-40e7-9eb2-f38895b401e4",
   "metadata": {},
   "source": [
    "With the add tool done, let’s move on to the definitions for our multiply and square tools.\n",
    "\n",
    "Please below define both `multiply` and `square` functiosn as tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f88dab85-6535-4613-b8e3-402367745e1a",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 21,
    "lastExecutedAt": 1735099046754,
    "lastExecutedByKernel": "c816e52d-560a-49bb-84da-f898f6c17d63",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "@tool\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\n\n@tool\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\n\n@tool\ndef square(a) -> int:\n    \"\"\"Calculates the square of a number.\"\"\"\n    a = int(a)\n    return a * a"
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def square(a: int) -> int:\n",
    "    \"\"\"Square a number.\"\"\"\n",
    "    return a ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60779ad4-450e-4d74-9fdd-b34710c1dc01",
   "metadata": {},
   "source": [
    "And that is it, simple as that.\n",
    "\n",
    "So we have defined our own three [custom tools](https://python.langchain.com/docs/modules/tools/custom_tools/). A more common use case might be to use some of the already provided and existing tools in LangChain, which you can see [here](https://python.langchain.com/docs/integrations/tools/). However, at the source code level, they would all be built and defined using a similar methods as described above.\n",
    "\n",
    "And that is it as far as our Tools our concerned. Now time to combine our tools into a Toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5aa604-11c5-4b1c-ae8a-35384812f7f3",
   "metadata": {},
   "source": [
    "## The Toolkit\n",
    "Toolkits sound fancy, but they are actually very simple. They are literally just a a list of tools. We can define our toolkit as a list of tools.\n",
    "\n",
    "In the list below, fill in the blanks with the name of the tools you defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0f30983-8045-4cb2-9e89-7a9c32af23d2",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 47,
    "lastExecutedAt": 1735099889428,
    "lastExecutedByKernel": "c816e52d-560a-49bb-84da-f898f6c17d63",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "toolkit = [add, multiply, square]"
   },
   "outputs": [],
   "source": [
    "toolkit = [add, multiply, square]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffcc542-d6a1-4573-abf8-e980f29951de",
   "metadata": {},
   "source": [
    "And that’s it. Really straightforward, and nothing to get confused over.\n",
    "\n",
    "Usually Toolkits are groups of tools that are useful together, and would be helpful for agents trying to carry out certain kinds of tasks. For example an SQLToolkit might contain a tool for generating an SQL query, validating an SQL query, and executing an SQL query.\n",
    "\n",
    "The [Integrations Toolkit](https://python.langchain.com/docs/integrations/toolkits/) page on the LangChain docs has a large list of toolkits developed by the community that might be useful for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dca6de1-0a0d-4134-9a60-ddde9e49fb9f",
   "metadata": {},
   "source": [
    "## The LLM\n",
    "\n",
    "As mentioned above, an LLM is the brains of an agent. It decides which tools to call based on the question passed to it, what are the best next steps to take based on a tools description. It also decides when it has reached its final answer, and is ready to return that to the user.\n",
    "\n",
    "Let’s setup the LLM here. The ChatOpenAI receives two parameters `model` and `temperature`. As the model you can use `\"gpt-3.5-turbo-1106\"`. Temperature typically ranges from -1 to 1, you can choose any temperature value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0f21c0a-3e35-4f06-a534-3a5c47242d21",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 54,
    "lastExecutedAt": 1735099965860,
    "lastExecutedByKernel": "c816e52d-560a-49bb-84da-f898f6c17d63",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0)"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model = \"gpt-3.5-turbo-1106\", \n",
    "    temperature=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c197daa5-a627-4b47-bcfd-3cc73c544029",
   "metadata": {},
   "source": [
    "## The Prompt\n",
    "Lastly we need a prompt to pass into our agent, so it has a general idea about what kind of agent it is, and what sorts of tasks it should solve.\n",
    "\n",
    "Our agent requires a ChatPromptTemplate to work (more on that later). This is what a barebones ChatPromptTemplate looks like. The main part we care about is the system prompt, and the rest are just the default settings we are required to pass in.\n",
    "\n",
    "In our prompt, we have included a sample answer, showing the agent how we want it to return the answer only, and not any descriptive text along with the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5422c30-2489-468b-90f7-7ae503ebefe1",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 54,
    "lastExecutedAt": 1735099083712,
    "lastExecutedByKernel": "c816e52d-560a-49bb-84da-f898f6c17d63",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"\"\"You are a mathematical assistant. Use your tools to answer questions.\n         If you do not have a tool to answer the question, say so.\n\n        Return only the answers. e.g\n        Human: What is 1 + 1?\n        AI: 2\n        \"\"\"),\n        MessagesPlaceholder(\"chat_history\", optional=True),\n        (\"human\", \"{input}\"),\n        MessagesPlaceholder(\"agent_scratchpad\"),\n    ]\n)\n"
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"You are a mathematical assistant. Use your tools to answer questions.\n",
    "         If you do not have a tool to answer the question, say so.\n",
    "\n",
    "        Return only the answers. e.g\n",
    "        Human: What is 1 + 1?\n",
    "        AI: 2\n",
    "        \"\"\"),\n",
    "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "        (\"human\", \"{input}\"),\n",
    "        MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a922868f-f65f-49a5-82dc-4012f0b10a93",
   "metadata": {},
   "source": [
    "That is it. We have setup our Tools and Toolkit, which our agent will need as part of its setup, so its knows what are the types of actions and capabilities it has at its disposal. And we have also setup the LLM and system prompt.\n",
    "\n",
    "Now for the fun part. Setting up our Agent!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb50724-1bc1-4d2d-b5f5-c3e256ba4219",
   "metadata": {},
   "source": [
    "## The Agent\n",
    "\n",
    "LangChain has a number of different agents types that can be created, with different reasoning powers and abilities. We will be using the most capable and powerful agent currently available, the OpenAI Tools agent. As per the docs on the the OpenAI Tools agent, which uses newer OpenAI models also,\n",
    "\n",
    "> Newer OpenAI models have been fine-tuned to detect when one or more function(s) should be called and respond with the inputs that should be passed to the function(s). In an API call, you can describe functions and have the model intelligently choose to output a JSON object containing arguments to call these functions. The goal of the OpenAI tools APIs is to more reliably return valid and useful function calls than what can be done using a generic text completion or chat API.\n",
    "\n",
    "In other words this agents is good at generating the correct structure for calling functions, and is able to understand if more than one function (tool) might be needed for our task also. This agent also has the ability to call functions (tools) with multiple input parameters, just like ours do. Some agents can only work with functions that have a single input parameter.\n",
    "\n",
    "If you are familiar with OpenAI’s Function calling feature, where we can use the OpenAI LLM to generate the correct parameters to call a function with, the OpenAI Tools agent we are using here is leveraging some of that power in order to be able to call the correct tool, with the correct parameters.\n",
    "\n",
    "In order to setup an agent in LangChain, we need to use one of the factory methods provided for creating the agent of our choice.\n",
    "\n",
    "The factory method for creating an OpenAI tools agent is `create_openai_tools_agent()`. And it requires passing in the llm, tools and prompt we setup above. So let’s initialise our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a6aba85-c9a8-40ed-8323-48bec599b2b3",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 49,
    "lastExecutedAt": 1735100277948,
    "lastExecutedByKernel": "c816e52d-560a-49bb-84da-f898f6c17d63",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "agent = create_openai_tools_agent(llm, toolkit, prompt)"
   },
   "outputs": [],
   "source": [
    "agent = create_openai_tools_agent(llm, toolkit, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5487d587-a2f0-4508-8307-e6b5293f2e92",
   "metadata": {},
   "source": [
    "Finally, in order to run agents in LangChain, we cannot just call a “run” type method on them directly. They need to be run via an AgentExecutor.\n",
    "\n",
    "I am bringing up the Agent Executor only here at the end as I don’t think it’s a critical concept for understanding how the agents work, and bring it up at the start with everything else would just the whole thing seem more complicated than it needs to be, as well as distract from understanding some of the other more fundamental concepts.\n",
    "\n",
    "So, now that we are introducing it, an `AgentExecutor` acts as the runtime for agents in LangChain, and allow an agent to keep running until it is ready to return its final response to the user. In pseudo-code, the AgentExecutor’s are doing something along the lines of (pulled directly from the LangChain docs):\n",
    "```\n",
    "next_action = agent.get_action(...)\n",
    "while next_action != AgentFinish:\n",
    "    observation = run(next_action)\n",
    "    next_action = agent.get_action(..., next_action, observation)\n",
    "return next_action\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12af0ee-9146-400b-a479-63d2129685ed",
   "metadata": {},
   "source": [
    "So they are basically a while loop that keep’s calling the next action methods on the agent, until the agent has returned its final response.\n",
    "\n",
    "So, let us setup our agent inside the agent executor. We pass it the agent, and must also pass it the toolkit. And we are setting verbose to True so we can get an idea of what the agent is doing as it is processing our request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b138dff-57cf-4d0d-91bf-dc8e178bf6be",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 50,
    "lastExecutedAt": 1735100370024,
    "lastExecutedByKernel": "c816e52d-560a-49bb-84da-f898f6c17d63",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "agent_executor = AgentExecutor(agent=agent, tools=toolkit, verbose=True)"
   },
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(agent=agent, tools=toolkit, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42a71ea-f711-445c-9428-0ac01e6d1f16",
   "metadata": {},
   "source": [
    "And that is it. We are now ready to pass commands to our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83b8baa5-efc8-450f-8fa6-7542d6e376cd",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastExecutedByKernel": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 185,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `add` with `{'a': 1, 'b': 3}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m4\u001b[0m\u001b[32;1m\u001b[1;3mThe sum of 1 and 3 is 4.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The sum of 1 and 3 is 4.\n"
     ]
    }
   ],
   "source": [
    "result = agent_executor.invoke({\"input\": \"what is 1 + 3?\"})\n",
    "print(result['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1d81e7-5968-4cb6-a094-21f814910553",
   "metadata": {},
   "source": [
    "Since we have set verbose=True on the AgentExecutor, we can see the lines of Action our agent has taken. It has identified we should call the “add” tool, called the “add” tool with the required parameters, and returned us our result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e7ec9d-3b52-4ad8-83b6-1f24ae7300d5",
   "metadata": {},
   "source": [
    "Please make several further trials to ensure that the LLM chooses the correct tool each time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fa7704f-099f-459d-8c3b-fe27b63539e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `multiply` with `{'a': 2, 'b': 3}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m6\u001b[0m\u001b[32;1m\u001b[1;3m2 multiplied by 3 is equal to 6.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "2 multiplied by 3 is equal to 6.\n"
     ]
    }
   ],
   "source": [
    "result = agent_executor.invoke({\"input\": \"what is 2 * 3?\"})\n",
    "print(result['output'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
